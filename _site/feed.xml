<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Benjamin Bengfort</title>
<subtitle type="text">A graduate research assistant in Computer Science at UMD</subtitle>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://www.cs.umd.edu/~bengfort/feed.xml" />
<link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort" />
<updated>2014-11-20T10:22:03-05:00</updated>
<id>http://www.cs.umd.edu/~bengfort/</id>
<author>
  <name>Benjamin Bengfort</name>
  <uri>http://www.cs.umd.edu/~bengfort/</uri>
  <email>bengfort@cs.umd.edu</email>
</author>


<entry>
  <title type="html"><![CDATA[Scientific Reading]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/thoughts/scientific-reading/" />
  <id>http://www.cs.umd.edu/~bengfort/thoughts/scientific-reading</id>
  <published>2014-11-20T00:00:00-05:00</published>
  <updated>2014-11-20T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Managing the reading of scientific papers is a challenge that most graduate students face because, let’s face it, scientific papers are tough to read. I would say that the first year of graduate school for me was an exercise not in learning how to do research or even higher order computer science principles but rather how to decipher academic papers (generally, not just in a single discipline). I’m still not sure that I’m an expert in reading these types of papers, but I am far more comfortable cracking open a paper and reading it than I was before; and importantly, I’m much faster at reading these papers than when I started. &lt;/p&gt;

&lt;p&gt;Reading is the first step; comprehension is the second, especially when the papers are required to directly inform what you’re thinking either for simple research papers, for coursework, or so that you can come up with new ideas. When I learned to read, I was trained in simple exercises in reading comprehension (read a passage, respond to questions). Unfortunately, narrative reading comprehension is not the same as scientific comprehension because critical evaluations of academic papers only partially use fact based extraction. &lt;/p&gt;

&lt;p&gt;Consider if you were reading &lt;em&gt;Philosophiæ Naturalis Principia Mathematica&lt;/em&gt; (one of the first scientific papers although only cited 1,750 times according to &lt;a href=&quot;http://scholar.google.com/scholar?cites=14485044281113753117&amp;amp;as_sdt=20000005&amp;amp;sciodt=0,21&amp;amp;hl=en&quot;&gt;Google Scholar&lt;/a&gt;), comprehension questions such as “What is the inverse square law?” only reveal partial understanding of the text. Better questions would allow you to elucidate why the &lt;em&gt;Principia&lt;/em&gt; is important, what work it evolved from, and what future work is required as a basis. For example, it’s important because it demonstrates a new mathematical technique that we call calculus, and uses the calculus methodology to demonstrate a precise method for computing elliptical motion around the sun, thereby proving the existence of a gravitational force. A heliocentric model of the solar system, as well as work in optics, and the discovery of the inverse square law were crucial prior work. Finally, important future work involves discovering the medium through which gravity is transferred and why celestial bodies at great distances experience the instantaneous effects of gravity. &lt;/p&gt;

&lt;p&gt;Frankly, that level of comprehension is provided to me through the benefit of nearly 400 years of reading comprehension with apologetic discourse. You don’t get nearly the same thing with papers written in the last 7 years, reviewed most likely by graduate students who weren’t willing or able to give a high level of scrutiny during the peer reviw process. You might also think that in computer science there would be software to review and execute; but this is also simply not true. Very often code and data are not provided to repeat experiments in a meaningful way. We are left requring a mechanism to critically evaluate text. &lt;/p&gt;

&lt;p&gt;This semester I’m taking a class with &lt;a href=&quot;http://kelehers.me/pete/&quot;&gt;Pete Keleher&lt;/a&gt;, and as part of the course we are reading papers related to the subject matter (distributed storage systems). Dr. Keleher is having us write blog posts on every paper as part of our course participation grade, and in these blog posts, we are asked to comment on or discuss the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the problem that the paper is attempting to address?&lt;/li&gt;
  &lt;li&gt;What is innovative about this paper, or what is the contribution?&lt;/li&gt;
  &lt;li&gt;Critically evaluate the paper with both positive and negative comments. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Over the course of the semester, I’ve discovered that this simple methodology of writing 500 words or so commenting on the paper in that fashion has led to a better reading comprehension on my part. Not only that, I’ve become faster at reading papers, because now I’m reading to answer these specific questions, but not only answer the questions - but comment on them and write about it in a coherent way. I’m finding that I’m doing a first pass read of the paper at a deeper than skimming level to get a feel for the organization, the topics, and any critical knowledge required. After that, I do a Google search for the topic, or review the bibliography for any linked knowledge I can add (which was tough when I first started, but now I’m able to recognize links to common systems). Finally- I start to write, and as I write, I dive back into the paper to read or understand critical concepts that are part of my elucidation of the three points above. &lt;/p&gt;

&lt;p&gt;Needless to say, this seems like an extremely good habit to continue with as much reading as I can possibly get away with; and this blog seems like a perfect forum to publish my thoughts on other papers. And I’m hoping to make this a regular habit here. So stay tuned for more. &lt;/p&gt;

&lt;p&gt;This methodology is addressing another problem  I have as well: organization. Currently, I use &lt;a href=&quot;https://www.zotero.org/&quot;&gt;Zotero&lt;/a&gt; and &lt;a href=&quot;http://www.papershipapp.com/&quot;&gt;Papership&lt;/a&gt; to organize my research and reading. Once I get into a paper, I create a “reading list” of citations along with their abstracts; then some notes about their importance to the work I’m doing. Using the “blog post reading” methodology, I will have all of the pieces prior to the paper writing, pre-organized for my benefit. I beieve that this process will be effective to making me a better scientist, and a better writer; and will update you as I go on! The first paper that I will experiment on will be a paper on diversity in Computer Science that has been sent around the department. It seems like an excellent starting place before getting into papers that are related to my research!&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/thoughts/scientific-reading/&quot;&gt;Scientific Reading&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 20, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Entity Resolution for Big Data]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data/" />
  <id>http://www.cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data</id>
  <updated>2014-03-10 13:57:07 UTCT00:00:00-00:00</updated>
  <published>2014-03-10T00:00:00-04:00</published>
  
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Entity Resolution is becoming an important discipline in Computer Science
and in Big Data especially with the recent release of Google’s
&lt;a href=&quot;http://www.google.com/insidesearch/features/search/knowledge.html&quot; title=&quot;Google&#39;s Knowledge Graph&quot;&gt;Knowledge Graph&lt;/a&gt; and the open &lt;a href=&quot;http://www.freebase.com/&quot; title=&quot;The Freebase API&quot;&gt;Freebase API&lt;/a&gt;.
Therefore it is exceptionally timely that last year at KDD 2013, Dr. Lise
Getoor of the University of Maryland and Dr. Ashwin Machanavajjhala of
Duke University gave a tutorial on
&lt;a href=&quot;http://www.kdd.org/kdd2013/accepted-tutorials&quot; title=&quot;Accepted Tutorials at KDD 2013&quot;&gt;&lt;em&gt;Entity Resolution for Big Data&lt;/em&gt;&lt;/a&gt;. We were fortunate enough
to be invited to attend a run through workshop at the Center for Scientific
Computation and Mathematical Modeling at College Park, and wanted to
highlight some of the key points for those unable to attend. This post has
been reposted from other sources.&lt;/p&gt;

&lt;style&gt;
    article img {
        margin: 28px auto;
        padding: 0;
        text-align: center;
    }
    a.image-popup {
        width: 100%;
    }
&lt;/style&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;what-is-entity-resolution&quot;&gt;What is Entity Resolution?&lt;/h2&gt;

&lt;p&gt;Entity Resolution is the task of disambiguating manifestations of real
world entities in various records or mentions by linking and grouping. For
example, there could be different ways of addressing the same person in
text, different addresses for businesses, or photos of a particular object.
This clearly has many applications, particularly in government and public
health data, web search, comparison shoppping, law enforcement, and more.&lt;/p&gt;

&lt;p&gt;Additionally, as the volume and velocity of data grows, inference across
networks and semantic relationships between entities becomes a greater
challenge. Entity Resolution can reduce the complexity by proposing
canonicalized references to particular entities and deduplicating and
linking entities. As an example, consider the following example of a
coauthor network from bibliographic data used for InfoVis 2004.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/network_resolution.png&quot; title=&quot;Network Resolution&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/network_resolution.png&quot; alt=&quot;ER and Network Analysis&quot; title=&quot;Network Resolution&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Deduplication significantly reduced the complexity of the network from an
ninth order graph to a much simpler fourth order graph, of significantly
less size. Dr. Getoor’s work on &lt;a href=&quot;http://www.cs.umd.edu/projects/linqs/ddupe/&quot; title=&quot;D-Dupe: Interactive Data Deduplication&quot;&gt;D-Dupe, an interactive data deduplication tool,&lt;/a&gt;
demonstrates how an iterative entity resolution process could work for
social networks, an increasingly important and growing data set that seems
to trancend traditional stovepipes like Twitter and Facebook. Yet another
example is the multiport disambiguation of Traceroute output of routers,
something that when incorrectly analyzed led to the commentary of some
technologists about the fagility of the Internet- when in fact most
Internet backbones are well defined and hardened.&lt;/p&gt;

&lt;p&gt;However, there are significant challenges to the ER discipline, least of
which is the fact that there is no unified theory and ironically, ER
itself goes by many names! Other challenges like language ambiguity, poor
data entry, missing values, changing attributes and formatting, as well as
abbreviations and truncation mean that ER is a discpline that includes not
just databases and information retrieval, but also natural language
processing and machine learning.&lt;/p&gt;

&lt;p&gt;Scaling to big data just increases the challenge, as the need for
heterogeneity and cross-domain resolution become important features. As a
result, ER techniques must be parallel and efficient, in order to be used
in the context of Big Data techniques like MapReduce and distributed graph
databases.&lt;/p&gt;

&lt;h2 id=&quot;tasks-in-entity-resolution&quot;&gt;Tasks in Entity Resolution&lt;/h2&gt;

&lt;p&gt;Generically speaking we can discuss ER as follows: there exists in the real
world entities, and in the digital world, records and mentions of those
entities. The records and mentions may take many forms, but they all refer
to only a single real world entity. We can therefore discuss the ER problem
as one involving matching record pairs corresponding to the same entity,
and as a graph of related records/mentions to related entities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/problem_statement.png&quot; title=&quot;Abstract ER Problem&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/problem_statement.png&quot; alt=&quot;Abstract ER Problem&quot; title=&quot;Abstract ER Problem&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: the task of clustering the records or mentions that
correspond to the same entity. There is an intensional variant of this
task: to then compute the cluster representative for each entity. This is
commonly what we think of when we consider Entity Resolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Record Linkage&lt;/strong&gt;: a slightly different version of the task is to match
records from one deduplicated data store to another. This task is proposed
in the context of already normalized data particularly in relational
databases. However, in the context of Big Data, a one to one comparison of
every record is not optimal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference Matching&lt;/strong&gt;: in this task, we must match noisy records to clean
ones in a deduplicated reference table. Note that is assumed that two
identical records are matches, this task is related to the task of entity
disambiguation.&lt;/p&gt;

&lt;p&gt;Further, when relationships between entites are added, each of the three
problem statements must also take into account the relationships between
records/mentions. Entity resolution techniques must take into account these
relationships as they are significant to disambiguation of entities.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-entity-resolution&quot;&gt;Evaluation of Entity Resolution&lt;/h3&gt;

&lt;p&gt;A quick note on the evaluation of entity resolution. For &lt;em&gt;pairwise metrics&lt;/em&gt;
we consider Precision and Recall (e.g. F1 scores), as well as the
cardinality of the number of predicted matching pairs. &lt;em&gt;Cluster level metrics&lt;/em&gt;
take into account purity, completeness, and complexity. This includes
cluster-level precision and recall, closest cluster, MUC, Rand Index, etc.
However, there has been little work on the correct prediction of links.&lt;/p&gt;

&lt;h2 id=&quot;the-algorithms-of-entity-resolution&quot;&gt;The Algorithms of Entity Resolution&lt;/h2&gt;

&lt;p&gt;This section includes a brief overview of algorithmic basis proposed by
Lise and Ashwin to provide a context for the current state of the art of
Entity Resolution. In particular, they discussed Data Preparation, Pairwise
Matching, Algoritms in Record Linkage, Deduplication, and Canonicalization.
They also considered collective entity resolution algorithms, that I will
briefly mention. Of course, they went into more depth on this section than
I will, but I hope to provide a good overview.&lt;/p&gt;

&lt;h3 id=&quot;data-preparation&quot;&gt;Data Preparation&lt;/h3&gt;

&lt;p&gt;First the tasks of &lt;em&gt;schema and data normalization&lt;/em&gt; are required preparation
for any Entity Resolution Algorithms. In this task, schema attributes are
matched (e.g. contact number vs phone number), and compound attributes like
addresses are normalized. Data normalization involves converting all
strings to upper or lower case and removing whitespace. Data cleaning and
dictionary lookups are also important.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Initial data prep is a big part of the work; smart normalization can go
a long way!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal is to construct, for a pair of records, a “comparison vector” of
&lt;em&gt;similarity scores&lt;/em&gt; of each component attribute. Similarity scores can
simply be Boolean (match or non-match) or they can be real values with
distance functions. For example, Edit distance on textual attributes can
handle typographic errors. Jaccard coefficients and other distance metrics
can be used to compare sets. Even phonetic similarity can be used.&lt;/p&gt;

&lt;h3 id=&quot;pairwise-matching&quot;&gt;Pairwise Matching&lt;/h3&gt;

&lt;p&gt;After we have constructed a vector of component-wise similarities for a
pair of records, we must compute the probability that the pair of records
is a match. There are several methods for discovering the probability of a
match. Two simple proposals are to use a weighted sum or average of
component similarity scores, and use thresholds. However, it is extremely
hard to pick weights or tune thresholds. Another simple approach can be
rule based matching, but manual formulation of rule sets is difficult.&lt;/p&gt;

&lt;p&gt;One interesting technique is the Fellegi &amp;amp; Sunter Model: given a record
pair, &lt;code&gt;r = (x,y)&lt;/code&gt; the comparison vector, is γ. If &lt;code&gt;M&lt;/code&gt; is the set of
matching pairs of records and &lt;code&gt;U&lt;/code&gt; is the set of non-matching pairs of
records, linkage decisions are based on the probability of γ given &lt;code&gt;r&lt;/code&gt;
∋ &lt;code&gt;M&lt;/code&gt; divided by the probability of γ given &lt;code&gt;r&lt;/code&gt; ∋ &lt;code&gt;U&lt;/code&gt;.
Further, you can decide if a record is a match or not based on error bounds,
μ and λ that create thresholds for whether a record is a match,
a non-match, or it is simply uncertain.&lt;/p&gt;

&lt;p&gt;In practice, Fellegi &amp;amp; Sunter requires some knowledge of matches to
train the error bounds, therefore some form of supervised learning is
required. In general, there are severaal techniques for Machine Learning
algorithms that can be applied to ER - Decision Trees, SVNS, Ensembles of
Classifiers, Conditional Random Fields, etc. The issue is Training Set
Generation since there is an imbalance of classes: non-matches far
outnumber matches, and this can result in misclassification.&lt;/p&gt;

&lt;p&gt;Active Learning methods and Unsupervised/Semi-supervised techniques, are
used to attempt to circumvent the difficulties of traning set generation.
Lise and Ashwin propose Committee of Classifiers approaches and even
crowdsourcing to leverage active learning to build a training set.&lt;/p&gt;

&lt;h3 id=&quot;constraints&quot;&gt;Constraints&lt;/h3&gt;

&lt;p&gt;There are several important forms of constraints that are relevant to the
next sections, given a specific mention, Mi:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;: If M1 and M2 match, M2 and M3 match, then M1 and M3
must also match.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exclusivity&lt;/strong&gt;: If M1 matches with M2, then M3 cannot match with M2&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Functional Dependency&lt;/strong&gt;: If M1 and M2 match, then M3 and M4 must match.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For these broad classes of constraints there are positive and negative
evidences of specific constraints, e.g. there is a converse to the match
constraint for a non-match or vice-versa. You may even consider hard and
soft constraints for the constraint types, as well as extent: can the
constraint be applied globablly or locally?&lt;/p&gt;

&lt;p&gt;Based on these constraints, you can see that transitivity is the key to
deduplication. Exclusivity is the key to record linkage and functional
dependencies are used for data cleaning. Further constraints like
aggregate, subsumption, neighboord, etc. can be used in a domain specific
context.&lt;/p&gt;

&lt;h3 id=&quot;specific-algorithms-for-problem-domains&quot;&gt;Specific Algorithms for Problem Domains&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Record Linkage&lt;/strong&gt;: propagation through the exclusitivity constraint means
that the current best solution is &lt;em&gt;Weighted K-Partite Matching&lt;/em&gt;. Edges are
pairs between records from different data sets whose weights are the
pairwise match score. The general problem is NP-hard, therefore the common
optimization is to perform successive bipartite matching.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: propegation through transitivity leads to a clustering
based entity resolution. There are a variety of clustering algorithms, but
the common input is pairwise similarity graphs. Many clustering algorithms
may also require the construction of a &lt;em&gt;cluster representative&lt;/em&gt; or a
&lt;em&gt;canonical entity&lt;/em&gt;. Although heirarhcical clustering or nearest neighbor
can be used, the recommended approach is &lt;em&gt;Correlation Clustering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Correlation Clustering uses Integer Linear Programming to maximize a cost
function that places positive and negative benefits of clustering mentions
x,y together, such that the Transitive closure is satisfied. However,
solving ILP is NP-hard, therefore a number of heuristics are used to
approximate the cost function including Greedy BEST/FIRST/VOTE algorithms,
the Greedy PIVOT algorithm, and local search.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Canonicalization&lt;/strong&gt;: Selection of the mention or cluster representative
that contains the most information. This can be rule based (e.g. longest
length), or for set value attributes a UNION. Edit distance can be used
to determine a most representative centroid, or use “majority rule”. Other
approaches include the Stanford Entity Resolution Framework (blackbox).&lt;/p&gt;

&lt;h3 id=&quot;collective-approaches&quot;&gt;Collective Approaches&lt;/h3&gt;

&lt;p&gt;If the decision for cluster-membership depends on other clusters, we can
use a collective approach. Collective approaches include non-probabilisitic
approaches like similarity propegation, or probabilistic models including
generative frameworks, or simply a hybrid approach.&lt;/p&gt;

&lt;p&gt;Generative probabilistic approaches are based on directed models, where
dependecies match decisions in a generative manner. There are a variety of
approaches, notably based on LDA and Bayesian Networks. Undirected
probabilistic approaches use semantics based on Markov Networks, to the
advantage that this allows a declarative syntax based on first-order logic
to create constraints. Several suggested approaches include Conditional
Random Fields, Markov Logic Networks (MLNs), and Probabilistic Soft Logic.&lt;/p&gt;

&lt;p&gt;Probabilistic soft logic introduces &lt;em&gt;reverse predicate equivalence&lt;/em&gt; meaning
that the same relation with the same entity gives evidence of two entities
being the same. This is not true logically, but can allow us to predict
the probability of a match with truth values in [0,1]. The declarative
language is used to define a constrained continuous Markov random field
in first order logic, and we can use relaxed logical operators. This has
significant implication for scaling improvements.&lt;/p&gt;

&lt;h2 id=&quot;scaling-to-big-data&quot;&gt;Scaling to Big Data&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Blocking/Canopy Generation&lt;/strong&gt;: The first approach to scaling to big data
is to use a blocking approach. Consider a Naïve pariwise comparsion
with 1,000 business mentions from 1,000 citties. This is &lt;em&gt;1 trillion&lt;/em&gt;
comparisons, which is 11.6 days for a microsecond comparison! However, we
know that business mentions are probably city-specific, therefore only
comparing mentions within cities reduces the comparison space to the more
managable &lt;em&gt;1 billion&lt;/em&gt; comparisions, which takes only 16 minutes at the same
rate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/blocking.png&quot; title=&quot;Blocking Heuristic&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/blocking.png&quot; alt=&quot;Blocking Heuristic&quot; title=&quot;Blocking Heuristic&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Although some matches can be missed using blocking techniques, the key is
to select a blocking algorithm that minimizes the subtraction of the set
of matching pair records and those satisfying the blocking criterion as in
the Venn diagram above, and maximizes the intersection. Common blocking
algorithms include hash based blocking, similarity or neighborhood based
blocking, or creating complex blocking predicates by combining simple
blocking predicates. Another powerful technique is minHash or locality
sensitive hashing, which uses distance measures and performs very well, and
is recommended as the state of the art for blocking.&lt;/p&gt;

&lt;p&gt;The final approach to blocking is more inline with the clustering methods
of earlier approaches. In &lt;em&gt;Canopy Clustering&lt;/em&gt; a distance metric is selected
with two thresholds. By picking a random mention, we create a canopy using
the first threshold and we remove all mentions where the distance from the
centroid is less than the second threshold. We continue this process so
long as the set M is not empty. This approach is interesting because
mentions can be included in more than one canopy, and therefore reduces the
chance that the blocking method excludes an actual match.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/canopy.png&quot; title=&quot;Canopy Clustering&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/canopy.png&quot; alt=&quot;Canopy Clustering&quot; title=&quot;Canopy Clustering&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributed ER&lt;/strong&gt;: The MapReduce framework is very popular for large,
parallel tasks and there are several open source frameworks related to
Hadoop to apply to your application. MapReduce can be used with disjoint
blocking- e.g. in the Map Phase (per record computation) you compute the
blocks and associate with various reducers, and in the reduce phase (global
computation) you can perform the pairwise matching. Several other challenges
relating to implementations with MapReduce are discussed in the text and
can be reviewed during implementations of ER in MapReduce.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Entity resolution is becoming an increasingly important task as linked data
grows, and the requirement for graph based reasoning extends beyond
theoritical applications. With the advent of big data computations, this
need has become even more prevalent. Much research has been done in the
area in seperately named fields, and the tutorial by Dr. Getoor and Dr.
Machanavajjhala succinctly highlight the current state of the art and gives
a general sense of future work.&lt;/p&gt;

&lt;p&gt;Specifically, there needs to be a unified approach to the theory which can
give relational learning bounds. Since Entity Resolution is often part of
bigger inference applications, there needs to be a joint approach to
information extraction, and a characterization of how the success (or
errors) affects larger reasoning quality. Similarly, there is a need for
large, real-world datasets with ground truth to establish benchmarks for
performance.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;This has been a summary of the work done by Dr. Lise Getoor and Dr. Ashwin
Machanavajjhala for their Tutorial entitled &lt;em&gt;Entity Resolution for Big Data&lt;/em&gt;,
accepted at KDD 2013 in Chicago, Il.&lt;/p&gt;

&lt;p&gt;You can find the complete tutorial slides at:
&lt;a href=&quot;http://www.cs.umd.edu/~getoor/Tutorials/ER_KDD2013.pdf&quot; title=&quot;Tutorial Slides&quot;&gt;http://www.cs.umd.edu/~getoor/Tutorials/ER_KDD2013.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Special thanks to Lise and Ashwin for hosting Cobrain during their
run-through of the presentation.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data/&quot;&gt;Entity Resolution for Big Data&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on March 10, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Goal Driven Mixed-Initiative Systems with Collaborative Filtering]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering/" />
  <id>http://www.cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering</id>
  <updated>2014-02-27 15:32:10 UTCT00:00:00-00:00</updated>
  <published>2014-02-27T00:00:00-05:00</published>
  
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Mixed-initiative systems combine the efforts of human actors with artificial agents to improve the performance of both participants in some mutual functionality. For example, an intelligent search application can be viewed as a mixed-initiative system where a human uses token strings or a special query language to enact searches that are carried out by an ensemble of statistical models which deliver intelligent results. In return, they benefit from the positive feedback of the user, e.g. which links are clicked, or how many queries it takes to find a result. &lt;/p&gt;

&lt;p&gt;A mixed-initiative system is said to be goal-driven when predefined goals are used to affect the behavior of both the human user and the underlying agent. In our search example, we could talk about an “academic goal” where the user is specifically looking for research materials. In this case, the types of queries made by the user will be influenced by this goal, and the results or domain of the agent should also take this goal into account. Additionally, the agent should seek to recover from failures or anomalies by generating its own goals. By using goals, we can effectively communicate bidirectional intent as well as explain failure in mixed-initiative systems. &lt;/p&gt;

&lt;p&gt;A third, interesting dimension comes into play when there are many human users participating in a mixed-initiative system. Namely, user behavior coupled with their goals creates a latent semantic network which can be accessed to improve mixed-initiative interactions through recommendation algorithms. In the next sections, I will describe the novel application of collaborative filtering algorithms improved with goal-dimensionality to mixed-initiative systems to both allow agents to provide other human help to a human user (goal combination) as well to elicit or change the goals of a human user in real time to improve their efficacy (goal generation). &lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering-for-goal-combination&quot;&gt;Collaborative Filtering for Goal Combination&lt;/h2&gt;

&lt;p&gt;Consider an opening scene from the 1970s TV version of Mission Impossible: Peter Graves pours over the dossiers of IMF agents as the camera hones in on their skills: weapons-expert, master of disguise, a navy diver. He carefully selects the perfect team for his mission as the burnt remnants of the mission tape smolder on the table. He knows that he will only succeed in his mission if he combines the expertise of his agent in a specific mission-oriented configuration.&lt;/p&gt;

&lt;p&gt;So too can our goal driven, mixed-initiative systems assist human users on their efforts, particularly in multi-domain systems. Each human user (or even possibly agent or model) has an expertise that is expressed through the goals of each user. A financial analyst will have goals related to their particular expertise, just like a language expert might also have goals related to a particular region. However, because a human user has many, changing goals- we can build a goal-to-goal similarity matrix using the goal-vector of the user; this matrix is the basis of item-centric collaborative filtering algorithms. &lt;/p&gt;

&lt;p&gt;The goal similarity matrix will produce clusters of related goals. The system can then use the goal cluster to discover other human analysts to recommend to the original user either in a passive context (provide hints based on the behavior of the other similar users) or in an active context- directly connecting the recommended user to the original user. Team selection can also be made on this basis- selection of members from related clusters to improve the overall effectiveness of the human team. Because recommended users will have different goals than the original user, this activity can be seen as goal combination. Either way, the human user will experience highly personalized guidance on part of the system through these recommendations.&lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering-for-goal-generation&quot;&gt;Collaborative Filtering for Goal Generation&lt;/h2&gt;

&lt;p&gt;As mentioned in the previous section, a human analyst (or agent’s) goals change over time- either as (1) the result of a new mission, (2) a change in world knowledge or the environment, or (3) in order to explain or recover from failure. It would be useful for a mixed-initiative system to anticipate these changes in goals particularly in order to adapt to (2) or (3). This will lead to a much smoother transition between goals, as well as improve the efficacy of a system that has to respond to a dynamic environment.&lt;/p&gt;

&lt;p&gt;Changing the goals of the analyst in real-time is possible through the novel use of our goal-based collaborative filtering with an additional dimension- the changing state of the behavior of the user. Our similarity matrix is now not only goal-to-goal, but instead goal-to-goal-to-state. This similarity matrix can then be decomposed to a series of class based goal-to-state similarity matrices. As the state of the user changes (e.g. the nature or classification of the behavior in terms of nearest-similar goals) these more similar goals are presented to the user in order to influence the user’s behavior towards more fruitful behavior (or to allow the user to avoid potential pitfalls when they recognize an ineffective goal).&lt;/p&gt;

&lt;p&gt;In this manner, the system can affect the behavior of the user before the user is aware of the need for a change in goals. This results in a highly personalized system that is bidirectional. Not only can the system use the behavior state to recommend goals and influence human behavior, the system can also use the goals to modify the results based on positive feedback from other users with similar goals and similar behavior. For instance, the selection of a recommended goal is a positive feedback mechanism that can be used in active learning when computing goal-state similarity matrices. &lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering/&quot;&gt;Goal Driven Mixed-Initiative Systems with Collaborative Filtering&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 27, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Getting Moved In]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/news/moving-in/" />
  <id>http://www.cs.umd.edu/~bengfort/news/moving-in</id>
  <published>2014-01-16T19:59:24Z</published>
  <updated>2014-01-16T19:59:24Z</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Hi, I’m Ben. Today, for the fourth time, I’m starting over on my PhD. I’ve
moved into my office in A.V. Williams, and I’m feeling like a veteran
entering an extended tour. But I’m here, I’m ready to work, so let’s get
researching!&lt;/p&gt;

&lt;p&gt;Also, I do Python.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#=&amp;gt; prints &amp;#39;[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&amp;#39; to STDOUT.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I build this site with &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;, which builds the static site from
Markdown in a repository.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/news/moving-in/&quot;&gt;Getting Moved In&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on January 16, 2014.&lt;/p&gt;</content>
</entry>

</feed>
