<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Benjamin Bengfort</title>
<subtitle type="text">A graduate research assistant in Computer Science at UMD</subtitle>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://cs.umd.edu/~bengfort/feed.xml" />
<link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort" />
<updated>2017-07-27T15:14:33-04:00</updated>
<id>http://cs.umd.edu/~bengfort/</id>
<author>
  <name>Benjamin Bengfort</name>
  <uri>http://cs.umd.edu/~bengfort/</uri>
  <email>bengfort@cs.umd.edu</email>
</author>


<entry>
  <title type="html"><![CDATA[Reading &ldquo;Harmony: Towards Automated Self-Adaptive Consistency in Cloud Storage&rdquo;]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/scientific-reading/harmony/" />
  <id>http://cs.umd.edu/~bengfort/scientific-reading/harmony</id>
  <published>2016-09-07T00:00:00-04:00</published>
  <updated>2016-09-07T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;blockquote&gt;
  &lt;p&gt;&lt;small&gt;H.-E. Chihoub, S. Ibrahim, G. Antoniu, and M. S. Perez, “Harmony: Towards automated self-adaptive consistency in cloud storage,” in 2012 IEEE International Conference on Cluster Computing, 2012, pp. 293–301.&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;In just a few years cloud computing has become a very popular paradigm and a business success story, with storage being one of the key features. To achieve high data availability, cloud storage services rely on replication. In this context, one major challenge is data consistency. In contrast to traditional approaches that are mostly based on strong consistency, many cloud storage services opt for weaker consistency models in order to achieve better availability and performance. This comes at the cost of a high probability of stale data being read, as the replicas involved in the reads may not always have the most recent write. In this paper, we propose a novel approach, named Harmony, which adaptively tunes the consistency level at run-time according to the application requirements. The key idea behind Harmony is an intelligent estimation model of stale reads, allowing to elastically scale up or down the number of replicas involved in read operations to maintain a low (possibly zero) tolerable fraction of stale reads. As a result, Harmony can meet the desired consistency of the applications while achieving good performance. We have implemented Harmony and performed extensive evaluations with the Cassandra cloud storage on Grid’5000 testbed and on Amazon EC2. The results show that Harmony can achieve good performance without exceeding the tolerated number of stale reads. For instance, in contrast to the static eventual consistency used in Cassandra, Harmony reduces the stale data being read by almost 80% while adding only minimal latency. Meanwhile, it improves the throughput of the system by 45% while maintaining the desired consistency requirements of the applications when compared to the strong consistency model in Cassandra.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;critical-reading&quot;&gt;Critical Reading&lt;/h2&gt;

&lt;p&gt;The Harmony paper is the second in the series of posts I’m writing that consider adaptive, hybrid, or continuous consistency models particularly with respect to the research I’m currently pursuing on Federated consistency.&lt;/p&gt;

&lt;p&gt;Chihoub et al. start with the novel premise that while replication and eventual consistency provide high availability, the performance of such systems (Dynamo, Cassandra, BigTable, PNUTS, and HBase) is too costly in terms of inconsistency. They first cite a statistic that a single hour of down time for a credit card authorization system could cost between $2.2M-3.1M. However, under heavy reads and writes some systems may return up to 66.61% of stale reads. This is alarming because the likelihood is that two out of three reads are useless. They present therefore a consistency model that focuses on stale reads as the primary inconsistency metric and propose an adaptive mechanism called Harmony to address stale reads.&lt;/p&gt;

&lt;p&gt;Their solution is to create an adaptive framework that sits on top of Cassandra. Applications can specify consistency in terms of the % of stale reads they are willing to tolerate: 0% stale reads means sequential consistency and 100% stale reads means letting the reigns go on eventual consistency. Most applications will choose some percent in the middle, however defining consistency this way allows applications to specify consistency along a continuum. Once specified, Harmony takes advantage of a Cassandra-specific feature that allows each access to specify the level of consistency. Normal Cassandra consistency settings are ONE or ALL, but you can also specify the number of nodes in the quorum required for an access.  Through real time monitoring of average read and write latencies, the system comes up with a likelihood of a stale read via a Poisson process model and uses that likelihood to compute the number of replicas required for the access such that the number of stale reads given the current likelihood is less than or equal to the application setting.&lt;/p&gt;

&lt;p&gt;In evaluating their framework, Chihoub et al. showed that they could split the middle ground between “eventual” (Cassandra ONE consistency) and “strong” (Cassandra ALL consistency) by using Harmony-80% and Harmon-40% settings, thereby allowing consistency tuning. However, they also showed that as the number of threads increases, throughput decreases so that the eventual systems become as bad as the strong systems. Potentially this has to do with the implementation (Python 2.7) of adaptive consistency not being able to keep up, or perhaps it indicates a systematic problem in larger distributed systems. Overall, stale reads is an interesting approach to minimizing inconsistencies, and I think this paper did well to show adaptivity in real time. However, this model only works in Cassandra and Dynamo, and no consistency checking in terms of ordering was done.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/scientific-reading/harmony/&quot;&gt;Reading &ldquo;Harmony: Towards Automated Self-Adaptive Consistency in Cloud Storage&rdquo;&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on September 07, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Reading &ldquo;Metastorage: A Federated Cloud Storage System to Manage Consistency-Latency Tradeoffs&rdquo;]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/scientific-reading/metastorage/" />
  <id>http://cs.umd.edu/~bengfort/scientific-reading/metastorage</id>
  <published>2016-08-30T00:00:00-04:00</published>
  <updated>2016-08-30T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;blockquote&gt;
  &lt;p&gt;&lt;small&gt;D. Bermbach, M. Klems, S. Tai, and M. Menzel, “Metastorage: A federated cloud storage system to manage consistency-latency tradeoffs,” in Cloud Computing (CLOUD), 2011 IEEE International Conference on, 2011, pp. 452–459.&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;Cost and scalability benefits of Cloud storage services are apparent. However, selecting a single storage service provider limits availability and scalability to the selected provider and may further cause a vendor lock-in effect. In this paper, we present MetaStorage, a federated Cloud storage system that can integrate diverse Cloud storage providers. MetaStorage is a highly available and scalable distributed hash table that replicates data on top of diverse storage services. MetaStorage reuses mechanisms from Amazon’s Dynamo for cross-provider replication and hence introduces a novel approach to manage consistency-latency tradeoffs by extending the traditional quorum (N,R,W) configurations to an (N&lt;sub&gt;&lt;small&gt;P&lt;/small&gt;&lt;/sub&gt;,R,W) scheme that includes different providers as an additional dimension. With MetaStorage, new means to control consistency-latency tradeoffs are introduced.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;critical-reading&quot;&gt;Critical Reading&lt;/h2&gt;

&lt;p&gt;The primary concern of Bermbach et al. seems to be vendor lock in to specific cloud services and the possibility of a single point of failure via downtimes or even going out of business. However, they also identify the fact that cloud services provide a variety of consistency and performance guarantees that have an impact on consistency due to latency. To solve these problems they present a system called MetaStorage which federates several cloud services with a focus on ensuring that MetaStorage is as available as the underlying service.&lt;/p&gt;

&lt;p&gt;MetaStorage performs GET and PUT requests using an (N&lt;sub&gt;&lt;small&gt;P&lt;/small&gt;&lt;/sub&gt;,R,W) policy where N is specified not as the number of replica servers but rather the number of providers. Distribution occurs by a list of preferences based on the hash of the key (e.g. a DHT). MetaStorage maintains message queues with each provider and for requests submits the access to R or W replicas in order of the preference, continuing until enough replicas allow for a response. For GET in particular, if R identical responses come in, that response is returned and for any nodes that don’t return that response, they are repaired with that value. If R reads cannot agree, all responses are returned or an error is returned with as many responses as available – thus delegating to the client the handling of inconsistency. Hinted handoffs ensure that all providers receive writes even after a response has been sent.&lt;/p&gt;

&lt;p&gt;Crucially, in order to ensure that there can be multiple distributors handling requests, some coordination is required – particularly to ensure all nodes maintain the same preference list. Rather than choosing to use a consensus approach to coordination, MetaStorage implements a lightweight, semi-centralized approach by giving all coordinators a complete list of all other coordinators in the system. The master is assumed to be at the top of the list; if the master cannot be reached, it is popped off the list and the next coordinator is selected. As coordinators come online, they are appended to the bottom of the list. Because the list is ordered and as long as every coordinator knows of more than two other coordinators, every replica is guaranteed to choose the same master. Configuration changes then can be made by allowing the master to wait until all currently queued requests are complete, holding all distributor nodes while the configuration is applied, then resuming operation.&lt;/p&gt;

&lt;p&gt;MetaStorage is certainly interesting as it is a first attempt to federate different consistency models; however MetaStorage can only guarantee eventual consistency both because the underlying storage mechanism is eventually consistent and because of the Dynamo-like policy they use for replication. Moreover while they cite an inconsistency window of 0.09ms (further reinforcing that eventual is fast enough) they show that the latency overhead is 300ms in a single availability region. They have performed no studies that discus geographic or wide area replication.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/scientific-reading/metastorage/&quot;&gt;Reading &ldquo;Metastorage: A Federated Cloud Storage System to Manage Consistency-Latency Tradeoffs&rdquo;&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on August 30, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Visualizing Distributed Systems]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/research/visualizing-distributed-systems/" />
  <id>http://cs.umd.edu/~bengfort/research/visualizing-distributed-systems</id>
  <published>2016-04-26T00:00:00-04:00</published>
  <updated>2016-04-26T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;As I’ve dug into my distributed systems research, one question keeps coming up:
&lt;em&gt;“How do you visualize distributed systems?”&lt;/em&gt; Distributed systems are &lt;a href=&quot;https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/&quot;&gt;hard&lt;/a&gt;, so it feels like being able to visualize the data flow would go a long way to understanding them in detail and avoiding bugs. Unfortunately, the same things that make architecting distributed systems difficult also make them hard to visualize.&lt;/p&gt;

&lt;p&gt;I don’t have an answer to this question, unfortunately. However, in this post I’d like to state my requirements and highlight some visualizations that I think are important. Hopefully this will be the start of a more complete investigation or at least allow others to comment on what they’re doing and whether or not visualization is important.&lt;/p&gt;

&lt;h2 id=&quot;static-visualization&quot;&gt;Static Visualization&lt;/h2&gt;

&lt;p&gt;A distributed system can loosely be described as multiple instances of a software program running on different machines that react to events. These events can be either external (a user making a request) or internal (handling requests from other instances). The collective individual behavior of each node informs how the entire system behaves.&lt;/p&gt;

&lt;p&gt;One high level view of the design of a system looks at the propagation of events, or messages being sent between nodes in the distributed system. This can be visualized using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Message_sequence_chart&quot;&gt;message sequence chart&lt;/a&gt; which embeds the time flow of a system and displays the interaction between nodes as they generate messages in reaction to received messages.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~dga/papers/epaxos-sosp2013.pdf&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/2016-04-26-epaxos-message-flow.png&quot; alt=&quot;Paxos Message Flow Diagram&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center; font-weight: bold; padding-bottom:10px&quot;&gt;&lt;small&gt;Message flow diagram for EPaxos&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;In the message sequence chart, every lane represents a single replica and arrows between them represent message passing and receipt order. Often, crossed arrows represent the difficulty in determining the &lt;em&gt;happens before&lt;/em&gt; relationship with respect to message order. These charts are good at defining a single situation and the reaction of the system, but do not do a good job at describing the general interaction. How do we describe a system in terms of the decisions it must make in reaction to received events that might be unordered?&lt;/p&gt;

&lt;p&gt;One method of designing a distributed system is to consider the design of only a &lt;em&gt;single&lt;/em&gt; instance. Each instance reacts to events (messages) then can update their state or do some work, and generate messages of their own. The receipt and sending of messages defines the collective behavior. This is a simplification of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Actor_model&quot;&gt;actor model&lt;/a&gt; of distributed computing. This seems like it might make things a bit easier, because now we only have to visualize the behavior of a single instance, and describe message handling as a flow chart of decision making.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/images/2016-04-26-raft-message-flow.png&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/2016-04-26-raft-message-flow.png&quot; alt=&quot;Flow Chart of Raft Messages&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center; font-weight: bold; padding-bottom:10px&quot;&gt;&lt;small&gt;Raft message reaction flow chart&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The flow chart above represents one of the attempts I’ve made to describe how the Raft consensus protocol works from the perspective of a single replica server. Raft is generally understood to be one of the most understandable consensus protocols, and as such it should be easy to describe visually. Here, messages are represented as colored circles. Raft has two primary RPC messages: request vote and append entries, therefore the circles represent the send and receive events of both RPC messages and their responses (8 total message types).&lt;br /&gt;
Each RPC roughly has their own zone in the flow chart. State changes are represented by the purple boxes, decisions by diamonds, and actions by square boxes. As you can see the flow chart is not completely connected, but hopefully by following from a “send” node to a “recv” node, one can track how the system interacts over time as well as individual nodes.&lt;/p&gt;

&lt;p&gt;This visualization still needs a lot of help, however. It is complex, and doesn’t necessarily embed all the information of how the complete system handles failure or messages.&lt;/p&gt;

&lt;h2 id=&quot;interactive-visualization&quot;&gt;Interactive Visualization&lt;/h2&gt;

&lt;p&gt;The most interesting combination of message traffic and behavior that I’ve seen so far requires JavaScript to create a dynamic, interactive visualization. Here, the user can play with different scenarios to see how the distributed system will react to different events or scenarios. It visualizes both the decision making process of the replica servers, as well as the ordering of messages as they’re sent and received.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://raft.github.io/&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/2016-04-26-raftscope-replay-visualization.png&quot; alt=&quot;RaftScope Visualization&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center; font-weight: bold; padding-bottom:10px&quot;&gt;&lt;small&gt;RaftScope visualization of the Raft protocol.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;One of the first places I encountered this was the &lt;a href=&quot;https://raft.github.io/&quot;&gt;RaftScope visualization&lt;/a&gt;. Here colored balls with an arrow represent the messages themselves (responses are not filled). The state of each node is shown by the edge color (a timer for followers, dotted for candidates, and solid for leaders). The log of each replica server is also displayed to show how the log repairs itself and commits values.&lt;/p&gt;

&lt;p&gt;Moreover, users can also click on nodes and disable them, make “client requests”, pause, or otherwise modify their behavior. This allows custom scenarios to be constructed and interpreted similar to the message sequence diagram, but with more flexibility. The problem is that the entire protocol must be implemented in JavaScript in order to ensure correct visualization (and is therefore a non-trivial, non-development approach to explaining how a system works).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://thesecretlivesofdata.com/raft/&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/2016-04-26-secret-lives-of-data-raft-visualization.png&quot; alt=&quot;The Secret Lives of Data&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center; font-weight: bold; padding-bottom:10px&quot;&gt;&lt;small&gt;The Secret Lives of Data interactive Raft tutorial&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;This idea was taken one step further by &lt;a href=&quot;http://thesecretlivesofdata.com/raft/&quot;&gt;The Secret Lives of Data&lt;/a&gt;, which uses a tutorial style presentation to show in detail each phase of the Raft algorithm. This allows the visualization to show specific scenarios rather than force the user to design them. I hope to see more tutorials for different algorithms soon!&lt;/p&gt;

&lt;p&gt;These two examples inspired me to create my own interactive visualization for the work I’m doing on consistency fragmentation. I use a similar design of circles for messages interacting with nodes in a circular topology. Right now it is still unfinished, but I’ve at least put together an MVP of what it might look like.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://bbengfort.github.io/cloudscope/&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/2016-04-26-cloudscope-consistency-visualization.png&quot; alt=&quot;CloudScope Consistency Visualization&quot; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center; font-weight: bold; padding-bottom:10px&quot;&gt;&lt;small&gt;CloudScope interactive consistency fragmentation visualization&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;My goal is to feed the visualization actual traces from the backend simulation I’m writing using SimPy or from the logs of live systems. The visualization will be less interactive (in the sense you can’t create specific scenarios) but will hopefully give insight into what is going on in the real system and allow me easier development and architecture.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So I pose to you the following questions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Is visualization important to the architecture of distributed systems?&lt;/li&gt;
  &lt;li&gt;How can we implement better static and interactive visualizations?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Visualization is not part of my research, but I hope an important part of describing what is happening in the system. Any feedback would be appreciated!&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/research/visualizing-distributed-systems/&quot;&gt;Visualizing Distributed Systems&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on April 26, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Actors for Distributed Dataflow]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/research/actors-for-distributed-dataflow/" />
  <id>http://cs.umd.edu/~bengfort/research/actors-for-distributed-dataflow</id>
  <published>2015-12-30T00:00:00-05:00</published>
  <updated>2015-12-30T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Recently my research crew (since we’re all in different research groups) has become interested in the possibility of reimagining &lt;em&gt;actors&lt;/em&gt; for distributed computation. Actor based computation is actually pretty old-school, but it’s regaining popularity, especially with distributed computation becoming vital for big data processing. After Kostas’ advisor, Dr. Amol Deshpande introduced us to the &lt;em&gt;Orleans&lt;/em&gt; paper that uses virtual actors; we decided to investigate further. Our project involved creating a simulation using &lt;code&gt;Simpy&lt;/code&gt; to study actor patterns. The paper below discusses our initial findings.&lt;/p&gt;

&lt;p&gt;You can find the Github Repository with the code for our simulation here: &lt;a href=&quot;http://bit.ly/actors-simulation&quot;&gt;http://bit.ly/actors-simulation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Recently, the Actor model of concurrency in distributed systems has regained popularity as cluster computing frameworks for large scale analytics have started becoming mainstream. In particular, the use of virtual actors has shown to provide automatic scaling and load balancing through virtual actor properties of perpetual existence, automatic instantiation, and locale transparency. This programming model appears to be ideal for data processing of streams of unbounded data sets, in a computing architecture of live, online processing of data. In this paper, we present the communication patterns of three such data processing applications using sets (or “casts”) of Actors. We then model the communication behavior on a cluster using a simulation and show that the virtual actor model effectively encapsulates how a cluster should behave in response to variable volumes of data. We propose that this simulation strongly motivates future work towards generalizing virtual actor spaces for distributed computation.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Sensors, web logs, and other timely data sources are increasingly being used for  real-time or on-demand applications that utilize machine learning models to quickly make predictions and adapt to changes reflected by the input data. Timely sources present a challenge as they are streams of unbounded, occasionally unordered data sets that must be computed upon in an online, real-time fashion rather than in batch. When these data sources are also variable (e.g. the volume of traffic can occasionally spike) then the underlying computing framework must be able to adapt and balance the load or risk falling so far behind that the computation becomes meaningless. Scalability must be &lt;em&gt;effortless&lt;/em&gt;; it should “just work” by simply scaling out the cluster size, and be &lt;em&gt;adaptive&lt;/em&gt; so that the cluster can be fairly utilized by all available jobs.&lt;/p&gt;

&lt;p&gt;Because of the widespread adoption of distributed computing frameworks like MapReduce [1] and Spark [2], distributed computing in a cluster of (often virtualized) economic servers has become the default, general method of high performance data processing. These frameworks primarily abstract the details of parallelism and distributing computation away from the developer, and have allowed a wider community of programmers and scientists to take advantage of parallel algorithms and libraries. Machine learning in particular has come of age in this computing environment, and as a result there exist many high level libraries for fitting a wide array of models in parallel.&lt;/p&gt;

&lt;p&gt;As a result, tools for dealing with &lt;em&gt;streaming data&lt;/em&gt; (unbounded, unordered, variable data sources) have mostly been contextualized similarly to parallel machine learning algorithms — via descriptions of how data flows through the application. Tools like Spark Streaming [3], Storm [4], and Google DataFlow [5] implement a &lt;em&gt;data flow&lt;/em&gt; programming model, where analysis is described as a directed graph whose nodes represent a single computation, and whose edges represent the transmission of input and output values. Describing computation this way is simple, adaptable, and when mapped to a distributed topology, scalable. However, this model does not allow nodes to &lt;em&gt;communicate&lt;/em&gt;, nor does it provide any flexibility in the computational topology for error handling, transactional guarantees, consistency, or persistence.&lt;/p&gt;

&lt;p&gt;In response to this inflexibility of the data flow model, the &lt;em&gt;actor model&lt;/em&gt; [6],[7] has been re-emerging as an abstraction that allows for more versatility in communication between distributed processes while still being at a high enough level to abstract the details of communication within the cluster away from the programmer. Ironically, it is this model that underpins the more general data flow models [8], perhaps due to the fact that these applications are implemented in the Scala programming language, which by default uses actors for concurrency [9], [10]. Actors in this context are high level primitives that do not share state, making them ideal candidates for under-the-hood parallelism. However, a simple data structure alone is not enough for distributed computing; there must also be an execution and job management context. Orleans [11] presents the idea of a “virtual actor space”, analogous to virtual memory, such that actor activations (e.g. instances of a program) exist forever, are transparent to their locale, and are automatically instantiated and deactivated. Virtual actors therefore provide for automatic scaling and load balancing in the cluster, and we believe are a novel and suitable alternative to the data flow model in the context of online, streaming computation.&lt;/p&gt;

&lt;p&gt;In this paper we investigate the potential of virtual actors for distributed computing on streaming data by simulating a computing cluster which implements a &lt;em&gt;generalized virtual actor space&lt;/em&gt;. A generalized virtual actor space replaces the programing model of virtual actors with a daemon service that runs in the background of all nodes in the cluster; allocating resources to a variety of actor programs in an on-demand fashion such that the resources can scale as variability of data streams changes, while balancing load across many “always on” computations that must share resources. We have simulated this framework by analyzing the communication patterns of three real-time applications: a traditional data flow, an online recommendation application, and a solar weather forecasting application. We then present an analysis of how the virtual actor model behaved in simulation on the cluster and show that this model is a suitable substitute for the data flow model.&lt;/p&gt;

&lt;p&gt;The rest of this paper is organized as follows. In the first section we present the details of our distributed computing model using actors, and describe the generalized virtual actor space in detail. Following this, we describe the applications we analyzed, and how we derived a communication model from each type of application. Finally, we describe our simulation methodology, present our results, and conclude with a discussion and future work.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/actors-dataflow.png&quot; alt=&quot;Simulated communication patterns of actors for our three models.&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center; padding-bottom:18px;&quot;&gt;
    &lt;strong&gt;&lt;small&gt;Actual communications patterns of actors for our three communication models: dataflow (blue), NNMF (green), and machine learning (red). The color identifies the actor type, and the size the number of messages received. This communication graph was constructed on a simulated cluster of 64 nodes with 4 processes per node.&lt;/small&gt;&lt;/strong&gt;
&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Read the full paper&lt;/strong&gt;: &lt;a href=&quot;http://cs.umd.edu/~bengfort/papers/actors-dataflow.pdf&quot;&gt;Actors for Distributed Dataflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Github Repository&lt;/strong&gt;: &lt;a href=&quot;http://bit.ly/actors-simulation&quot;&gt;GVAS Actors Simulation&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/research/actors-for-distributed-dataflow/&quot;&gt;Actors for Distributed Dataflow&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on December 30, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Research Pivot]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/news/research-pivot/" />
  <id>http://cs.umd.edu/~bengfort/news/research-pivot</id>
  <published>2015-11-20T09:00:00-05:00</published>
  <updated>2015-11-20T09:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Once again I’ve made a very difficult and also very exciting choice to switch my research topic away from case based reasoning and knowledge goals to distributed storage systems. Today it’s official: I’ve received my RA letter to start work with &lt;a href=&quot;http://pete.kelehers.me/&quot;&gt;Dr. Pete Keleher&lt;/a&gt;, my new advisor. Starting on January 4, 2016: I will move forward on the implementation of a project that I started in his class in the Fall 2014 semester (incidentally, my favorite class at the University of Maryland). Hopefully this project will allow me to become much better at the &lt;a href=&quot;https://golang.org/&quot;&gt;Go programming language&lt;/a&gt;, my newly preferred systems programming language, and possibly also produce research papers so that I can graduate at some point.&lt;/p&gt;

&lt;h2 id=&quot;what-went-down&quot;&gt;What Went Down&lt;/h2&gt;

&lt;p&gt;As you might know if you’ve been following my research career, (or at the very least noticed the wide variety of topics on my publications page) I’ve had several advisors to this point. My personal statement even mentions that I’m extremely good luck to academic professionals, as shortly after advising me, they tend to get promotions and move to other universities! The honor role now includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cis-linux1.temple.edu/~xjdu/&quot;&gt;Dr. Xiao-jiang Du&lt;/a&gt; who worked with me while I studied hybrid wireless networks at NDSU and is now the Chair of Future Computing at Temple University.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://maxzhang.info/&quot;&gt;Dr. Weiyi Zhang&lt;/a&gt; with whom I finished my master’s thesis, now at AT&amp;amp;T Laboratories.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hass.rpi.edu/pl/faculty-staff-s17/nirenburg&quot;&gt;Dr. Sergei Nirenburg&lt;/a&gt; who encouraged me to continue my PhD and shift to Natural Language Processing and Artificial Intelligence and now has a joint position at RPI in Cognitive Science and Computer Science.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://mcox.org/&quot;&gt;Dr. Michael Cox&lt;/a&gt; who took me in to continue my research I started with Dr. Nirenburg at UMD, and is now a senior research scientist at Wright State Research Institute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I consider myself especially fortunate having had such distinguished mentors who have taught me how to conduct research and equipped me with a broad background in several computer science fields.&lt;/p&gt;

&lt;p&gt;My most recent advisor at Maryland, Dr. Cox, actually received his appointment last summer and moved shortly thereafter. I continued to work with him remotely for a year. However, as I have come closer to completing my requirements and move to my oral exam and proposal; the University of Maryland was concerned that I have a resident advisor in order to complete the process. I spoke with many professors at UMD who might take over as chair of my committee, but most felt that my work was too far outside of their research area. It was with great regret that Dr. Cox and I had to move forward in different directions.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next&lt;/h2&gt;

&lt;p&gt;I’ve long been interested in distributed systems and databases, and most of my professional work is a combination of distributed systems and machine learning. The move to work with Dr. Keleher seems like a great fit, and although it is a drastic shift from my previous work; it is very comfortable to me from an applied standpoint. Moreover, his class well prepared me for pursuing research in this area, and I don’t feel like I’m starting from scratch.&lt;/p&gt;

&lt;p&gt;In particular I will be looking at consistency in small-scale distributed file systems; e.g. those that users might use, not those that exist on a cluster. This is a fascinating topic that I hope will have a large impact. Hopefully I will also be able to publish significant enough results that I can move to my dissertation and wrap up in a timely fashion.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/news/research-pivot/&quot;&gt;Research Pivot&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 20, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Evolutionary Design of Self-Organizing Particle Systems for Collective Problem Solving]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/research/evolutionary-design-of-particle-swarms/" />
  <id>http://cs.umd.edu/~bengfort/research/evolutionary-design-of-particle-swarms</id>
  <published>2015-05-04T00:00:00-04:00</published>
  <updated>2015-05-04T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;iframe src=&quot;//www.slideshare.net/slideshow/embed_code/key/M99oimH59QL5V1&quot; width=&quot;595&quot; height=&quot;485&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; style=&quot;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&quot; allowfullscreen=&quot;&quot;&gt; &lt;/iframe&gt;
&lt;div style=&quot;margin-bottom:15px&quot;&gt; &lt;strong&gt; &lt;a href=&quot;//www.slideshare.net/BenjaminBengfort/evolutionary-design-of-swarms-ssci-2014&quot; title=&quot;Evolutionary Design of Swarms (SSCI 2014)&quot; target=&quot;_blank&quot;&gt;Evolutionary Design of Swarms (SSCI 2014)&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href=&quot;//www.slideshare.net/BenjaminBengfort&quot; target=&quot;_blank&quot;&gt;Benjamin Bengfort&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;

&lt;p&gt;In December, I attended IEEE SSCI 2014 - a symposium on computational intelligence where I presented work that my classmates and I put together for Jim Reggia’s class: &lt;em&gt;Artificial Life and Evolutionary Computation&lt;/em&gt;. The goal of the paper was to show off the use of evolutionary and genetic algorithms for &lt;em&gt;creativity&lt;/em&gt;, that is the use of a computer program to design a system without very much human input. I’ve attached the abstract, paper, and presentation from that conference here. However, the purpose of this post isn’t to point out the scientific contributions - but rather the practical ones.&lt;/p&gt;

&lt;p&gt;In order to do this project, we had to create a pretty extensive Python software base with the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A simulator that could both visually and computationally determine the fitness of a particle swarm&lt;/li&gt;
  &lt;li&gt;A multiprocess evolutionary mechanism for designing the particle swarms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the end our deployment used PyGame, Celery, and an Amazon EC2 2XL server which ran for a week to finish the computation. The paper doesn’t necessarily touch too much on the development effort required. Given the opportunity to do this again, I probably would have used Go or Cython to improve the performance (not much you can do in a single semester). This post collects both the Github repository and development environment with the scientific details. Hopefully it’s a useful reference!&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Using only simple rules for local interactions, groups
of agents can form self-organizing super-organisms or flocks
that show global emergent behavior. When agents are also
extended with memory and goals the resulting flock not only
demonstrates emergent behavior, but also collective intelligence:
the ability for the group to solve problems that might be
beyond the ability of the individual alone. Until now, research
has focused on the improvement of particle design for global
behavior; however, techniques for human-designed particles are
task-specific. In this paper we will demonstrate that evolutionary
computing techniques can be applied to design particles, not only
to optimize the parameters for movement but also the structure of
controlling finite state machines that enable collective intelligence.
The evolved design not only exhibits emergent, self-organizing
behavior but also significantly outperforms a human design in
a specific problem domain. The strategy of the evolved design
may be very different from what is intuitive to humans and
perhaps reflects more accurately how nature designs systems for
problem solving. Furthermore, evolutionary design of particles
for collective intelligence is more flexible and able to target a
wider array of problems either individually or as a whole.&lt;/p&gt;

&lt;h3 id=&quot;project&quot;&gt;Project&lt;/h3&gt;

&lt;p&gt;The full project is available on Github if you’d like to try it yourself. The Github repository contains instructions for how to install the dependencies, and how to run the simulator as well as the evolver. The dependencies might be the main issue, installing PyGame for the visual simulation can be tricky, and Celery is needed for the multi-process evolver. We will respond to any issues that you add as soon as we can!&lt;/p&gt;

&lt;style&gt;

    img {
        margin-top: 0px;
        margin-bottom: 20px;
    }

&lt;/style&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mclumd/swarm-simulator/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/mclumd/swarm-simulator.svg&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/mclumd/swarm-simulator/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/mclumd/swarm-simulator.svg&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/mclumd/swarm-simulator/network&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/mclumd/swarm-simulator.svg&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://raw.githubusercontent.com/mclumd/swarm-simulator/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The code itself is provided under the MIT license. The various badges describing the Github repo are above.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mclumd/swarm-simulator&quot;&gt;&lt;strong&gt;Try the project yourself on Github&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7011790&quot;&gt;&lt;strong&gt;Read the full paper on IEEE Xplore&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/BenjaminBengfort/evolutionary-design-of-swarms-ssci-2014&quot;&gt;&lt;strong&gt;View the presentation on Slideshare&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/research/evolutionary-design-of-particle-swarms/&quot;&gt;Evolutionary Design of Self-Organizing Particle Systems for Collective Problem Solving&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on May 04, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Visual Discovery of Communication Patterns in Email Networks]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/thoughts/visual-discovery-of-communication-patterns-in-email-networks/" />
  <id>http://cs.umd.edu/~bengfort/thoughts/visual-discovery-of-communication-patterns-in-email-networks</id>
  <published>2015-04-06T00:00:00-04:00</published>
  <updated>2015-04-06T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/gephi_analysis.png&quot; alt=&quot;Analysis of Betweenness Centrality&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center; padding-bottom:18px;&quot;&gt;
    &lt;strong&gt;&lt;small&gt;An analysis of key players in a large and small email network.&lt;/small&gt;&lt;/strong&gt;
&lt;/p&gt;

&lt;p&gt;In the age of social networks, it is very popular to analyze Twitter and Facebook to detect relationships and communities and to discover how information flows between groups. However, both Facebook and Twitter are examples of “performed” social networks: because a user has full control over who he or she “friends” or “follows,” he can artificially create a persona that he wishes the outside world to perceive him as. In fact, it has been well-studied that expressions of taste typically exclude the most critical groups to communication flows, and that social networks are merely the expressions of performed taste (Liu, 2007).&lt;/p&gt;

&lt;p&gt;Email, on the other hand, is used in every facet of modern life and typically includes and aggregates facets of ourselves that would otherwise be segregated — for example, professional and personal personas. Because we spend so much time writing, reading, and responding to email, it has become integrated into a communication fabric that far exceeds other media that may get more analytical attention like text messaging or social networks. Email, therefore, can be seen to embed a natural communication network, from which we can extract rich insights about actual communications and influences within a single ego network — the email inbox of a single user — without the bias of taste performance.&lt;/p&gt;

&lt;p&gt;In this paper, we will explore the visual analysis of two personal email networks, one from each of the authors, where one network is extremely large in terms of the number of nodes (email addresses) and the other network is extremely large in terms of the number of edges (cliquish, highly-connected email). We hope to show that if a user can visualize their own networks, they will better understand the dynamic nature of their communication in terms of key players, communities, and gaps in communication. Through such visual understanding, users may be better able to respond to and manage the constant flow of email messages meaningfully.&lt;/p&gt;

&lt;p&gt;We will use Gephi (Gephi, 2010) to visualize email networks that have been extracted from &lt;code&gt;mbox&lt;/code&gt; files via a Python tool written by the authors called &lt;code&gt;tribe&lt;/code&gt;, which serializes the network into GraphML (Brandes, 2010). Gephi is widely used for visual exploration of networks (Bastian, 2009) and includes many features for the statistical analysis of small to medium networks (McSweeney, 2009). Work of note demonstrates the analysis of dynamic network connections within Twitter conversations (Bruns, 2012), which shows how Gephi might be used for networks whose topology can rapidly shift, as in email. As part of our analysis we will critique Gephi and contrast it to NodeXL (Smith, 2010), another popular tool for visual network analysis.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Read the full article&lt;/strong&gt;: &lt;a href=&quot;http://cs.umd.edu/~bengfort/papers/visual-discovery-email-networks.pdf&quot;&gt;Visual Discovery of Communication Patterns in Email Networks&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/thoughts/visual-discovery-of-communication-patterns-in-email-networks/&quot;&gt;Visual Discovery of Communication Patterns in Email Networks&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on April 06, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Analyzing Response to Vehicular Incidents with Tableau]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/thoughts/analyzing-response-to-vehicular-incidents-with-tableau/" />
  <id>http://cs.umd.edu/~bengfort/thoughts/analyzing-response-to-vehicular-incidents-with-tableau</id>
  <published>2015-03-02T00:00:00-05:00</published>
  <updated>2015-03-02T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/dashboard.png&quot; alt=&quot;Tableau Dashboard of Traffic Incidents&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center; padding-bottom:18px;&quot;&gt;
    &lt;strong&gt;&lt;small&gt;Tableau Dashboard Visualizing Traffic Incidents&lt;/small&gt;&lt;/strong&gt;
&lt;/p&gt;

&lt;p&gt;In the State of Maryland, the State Highway Administration (MDSHA) oversees critical responses
to events and incidents that occur on the roadways, 24 hours a day - 7 days a week. Maryland has
some of the highest density traffic in the United States [4], and includes two major beltways (ring
roads) around Baltimore and Washington, I-695 and I-495 respectively. It is therefore critical that
first responders are prepared for changing road conditions and are able to plan well in advance for
management of routine and extraordinary events.&lt;/p&gt;

&lt;p&gt;At the University of Maryland, the Center for Advanced Transportation Technology (CATT) Laboratory
explores user and visualization driven techniques to solve complex challenges of transportation,
safety, and security for the Maryland region. Recently, data collection efforts have improved
and large scale probe networks have equipped traffic operations specialists with real time probe data
for dynamic monitoring. The CATT Lab has shown that with effective, real-time visualization
of traffic patterns and event analysis, traffic administrators are able to respond more quickly and
prepare in advance.&lt;/p&gt;

&lt;p&gt;With the support of Maryland State Highway Administration’s Incident Management System, the
CATT Laboratory has collected 287,712 events that Maryland Coordinated Highway Action Response
Teams (CHARTs) responded to from 2011 - 2014. In this paper we use Tableau to explore
patterns in historical responses to these events and critique Tableau’s effectiveness as a data analysis
tool for response preparedness. We hope to show that a commercial tool for general data analysis
like Tableau can provide insights from large amounts of data, and allow first responders to plan and
prepare for future events.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Read the full article&lt;/strong&gt;: &lt;a href=&quot;http://cs.umd.edu/~bengfort/papers/analyzing-response-vehicular.pdf&quot;&gt;Analyzing Response to Vehicular Incidents with Tableau&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/thoughts/analyzing-response-to-vehicular-incidents-with-tableau/&quot;&gt;Analyzing Response to Vehicular Incidents with Tableau&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on March 02, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Baxter Robots are Learning to Think, See, and Cook]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/thoughts/baxter-robots-are-learning-to-think-see-and-read/" />
  <id>http://cs.umd.edu/~bengfort/thoughts/baxter-robots-are-learning-to-think-see-and-read</id>
  <published>2015-02-26T00:00:00-05:00</published>
  <updated>2015-02-26T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/IMG_1955.JPG&quot; alt=&quot;Researchers at the University of Maryland explore practical AI with Baxter Robots&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center; padding-bottom:18px;&quot;&gt;
    &lt;strong&gt;&lt;small&gt;Researchers at the University of Maryland explore practical AI with Baxter Robots&lt;/small&gt;&lt;/strong&gt;
&lt;/p&gt;

&lt;p&gt;In April 2014, researchers at the University of Maryland acquired two Baxter Intera 3 humanoid robots manufactured by Rethink Robotics.  Baxter robots are well known for their use in micro-manufacturing as they are specially designed for repeated grasping and moving tasks in a stationary position.  While a Baxter’s multi-dimensional range of motion is important, What makes them especially interesting is their adaptability. Baxter robots are “trained” – not programmed – on work sites.  Human workers move the Baxter’s incredibly flexible robotic arms to “show” Baxter how to do a particular task. This ability, combined with Baxter’s powerful onboard processing, a suite of sensors, and smooth operating mechanisms makes Baxter ideal for research.&lt;/p&gt;

&lt;p&gt;In Computer Science, research groups have already begun to explore computer vision, machine learning, and artificial intelligence within a robotic context. Three research groups — the Computer Vision Labratory (CFAR), the Metacognitive Lab (MCL), and the Maryland Robotics Center — have been cooperating to produce practical results from their individual theoretical fields.  Simple tasks like picking up utensils and stirring bowls, pouring water in a moving container, and building structures from blocks have been quickly achieved thanks to this collaboration.  By studying the difficulties involved in teaching Baxter to perform these tasks, the research groups hope to solve larger theoretical challenges.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Read the full article&lt;/strong&gt;: &lt;a href=&quot;http://cs.umd.edu/~bengfort/papers/baxter-robots-learning.pdf&quot;&gt;Baxter Robots are Learning to Think, See, and Cook&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/thoughts/baxter-robots-are-learning-to-think-see-and-read/&quot;&gt;Baxter Robots are Learning to Think, See, and Cook&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 26, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Getting Started with Spark (in Python)]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/tutorials/getting-started-with-spark/" />
  <id>http://cs.umd.edu/~bengfort/tutorials/getting-started-with-spark</id>
  <published>2015-02-02T00:00:00-05:00</published>
  <updated>2015-02-02T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Hadoop is the standard tool for distributed computing across really large data sets and is the reason why you see “Big Data” on advertisements as you walk through the airport. It has become an operating system for Big Data, providing a rich ecosystem of tools and techniques that allow you to use a large cluster of relatively cheap commodity hardware to do computing at supercomputer scale. Two ideas from Google in 2003 and 2004 made Hadoop possible: a framework for distributed storage (The Google File System), which is implemented as HDFS in Hadoop, and a framework for distributed computing (MapReduce).&lt;/p&gt;

&lt;p&gt;These two ideas have been the prime drivers for the advent of scaling analytics, large scale machine learning, and other big data appliances for the last ten years! However, in technology terms, ten years is an incredibly long time, and there are some well-known limitations that exist, with MapReduce in particular. Notably, programming MapReduce is difficult. You have to chain Map and Reduce tasks together in multiple steps for most analytics. This has resulted in specialized systems for performing SQL-like computations or machine learning. Worse, MapReduce requires data to be serialized to disk between each step, which means that the I/O cost of a MapReduce job is high, making interactive analysis and iterative algorithms very expensive; and the thing is, almost all optimization and machine learning is iterative.&lt;/p&gt;

&lt;p&gt;To address these problems, Hadoop has been moving to a more general resource management framework for computation, YARN (Yet Another Resource Negotiator). YARN implements the next generation of MapReduce, but also allows applications to leverage distributed resources without having to compute with MapReduce. By generalizing the management of the cluster, research has moved toward generalizations of distributed computation, expanding the ideas first imagined in MapReduce.&lt;/p&gt;

&lt;p&gt;Spark is the first fast, general purpose distributed computing paradigm resulting from this shift and is gaining popularity rapidly. Spark extends the MapReduce model to support more types of computations using a functional programming paradigm, and it can cover a wide range of workflows that previously were implemented as specialized systems built on top of Hadoop. Spark uses in-memory caching to improve performance and, therefore, is fast enough to allow for interactive analysis (as though you were sitting on the Python interpreter, interacting with the cluster). Caching also improves the performance of iterative algorithms, which makes it great for data theoretic tasks, especially machine learning.&lt;/p&gt;

&lt;p&gt;In this post we will first discuss how to set up Spark to start easily performing analytics, either simply on your local machine or in a cluster on EC2. We then will explore Spark at an introductory level, moving towards an understanding of what Spark is and how it works (hopefully motivating further exploration). In the last two sections we will start to interact with Spark on the command line and then demo how to write a Spark application in Python and submit it to the cluster as a Spark job.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/tutorials/getting-started-with-spark/&quot;&gt;Getting Started with Spark (in Python)&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 02, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Reading &ldquo;Spanner: Google's Globally Distributed Database&rdquo;]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/scientific-reading/spanner/" />
  <id>http://cs.umd.edu/~bengfort/scientific-reading/spanner</id>
  <published>2014-11-28T00:00:00-05:00</published>
  <updated>2014-11-28T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;blockquote&gt;
  &lt;p&gt;&lt;small&gt;J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, and others, “Spanner: Google’s globally distributed database,” ACM Transactions on Computer Systems (TOCS), vol. 31, no. 3, p. 8, 2013.&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;Spanner is Google’s scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: non-blocking reads in the past, lock-free read-only transactions, and atomic schema changes, across all of Spanner.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;critical-reading&quot;&gt;Critical Reading&lt;/h2&gt;

&lt;p&gt;When dealing with data stores that contain mission critical or extremely valuable information that is used by many individuals all over the world, you want to ensure that the data remains consistent and is durable across data centers that span the globe. Data replication provides durability, clients accessing geographically close servers gives low latency, and because multiple replicas have data, failure can be easily handled. Creating a database system that manages cross-data center replication  that is synchronously replicated, however, is a major challenge especially when attempting to allow lock-free read only transactions and atomic, linear changes that are consistent and versioned.&lt;/p&gt;

&lt;p&gt;To date, the systems that we have discussed in class have focused on transactional invariants that give stronger semantics than simple eventual consistency to provide a data service that expands our understanding of CAP. These data systems have primarily been key-value stores, which simplifies the data storage expectations to look at transactional invariants (and explore DHTs and other techniques). However, from the application perspective, a key-value store is difficult to use for applications that have complex schemas that might be subject to change or for applications that are happy to lose some availability in favor of strong semantics for wide area consistency.&lt;/p&gt;

&lt;p&gt;Spanner is a globally distributed database that provides &lt;em&gt;external consistency&lt;/em&gt; between data centers and stores data in a schema based semi-relational data structure. Not only that, Spanner provides a versioned view of the data that allows for instantaneous snapshot isolation across any segment of the data. This versioned isolation allows Spanner to provide globally consistent reads of the database at a particular time allowing for lock-free read-only transactions (and therefore no communications overhead for consensus during these types of reads). Spanner also provides externally consistent reads and writes with a timestamp-based linear execution of transactions and two phase commits. Spanner is the first distributed database that provides global sharding and replication with strong consistency semantics.&lt;/p&gt;

&lt;p&gt;The primary innovation of Spanner is its implementation of timestamp management an linear serializability of transactions via a trusted global time, a system they call &lt;code&gt;TrueTime&lt;/code&gt;. &lt;code&gt;TrueTime&lt;/code&gt; represents timestamps as an interval, the timestamp +/- some uncertainty concerning the true time. The use of uncertainty means that Spanner will slow down to ensure that there is no local time conflict with the global system time and provide a reliable timestamp for each transaction in the system. The implementation of TrueTime involves the use of distributed master time servers that have GPS and atomic clocks and an polling daemon on clients that implements a time synchronization protocol with several masters and computes uncertainty of local clock drift.&lt;/p&gt;

&lt;p&gt;I was duly impressed with Spanner - not only does it provide a strong consistency model across a globally distributed system, it also implements a semi-relational interface that more applications can use with ease over NoSQL proposals (including a structured query language, schemas, and client-specific control of wide-area replication). Spanner is rightly robust (as it contains Google’s most valuable data, the advertising network) with an impressive latency, even using Paxos for consensus under the hood. However, this system is not practical as an open source, academic, or research system. Google calls a Spanner implementation a “universe” and there are only three Google Spanner “universes” and rightly so - multiple data centers with atomic and GPS clocks is an expensive proposition! However, because the innovation relies on strong universal time, and the use of Marzullo’s algorithm to ensure multiple timeservers give an accurate time stamp, it may be possible to rely on the strong global time invariant to explore other consistency models (much like GPS allows us to explore other scientific research related to things beyond navigation) so long as someone sets up a universally available &lt;code&gt;TrueTime&lt;/code&gt; protocol system that anyone can connect to!&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other Notes&lt;/h2&gt;

&lt;p&gt;This paper was my responsibility for presentation to the course, and to start the presentation, I was asked to play the video of the presentation by Wilson Hsieh at OSDI 2012 (links below). Wilson focused (rightly so) on &lt;code&gt;TrueTime&lt;/code&gt; after giving a clear example of why Spanner is important in the context of a social network. He skipped most of the implementation details of Spanner that were presented in the paper. My presentation focused primarily on the implementation details, since &lt;code&gt;TrueTime&lt;/code&gt; was discussed in detail, the links to the presentation I gave can also be found below.&lt;/p&gt;

&lt;p&gt;In my presentation, I couldn’t help compare Spanner to BigTable, another Google paper that I may have to blog about (and my systems research may as well be called Google distributed systems!). Like BigTable, Spanner uses a data structure called Tablets under the hood, essentially a bag of related key value pairs. Unlike BigTable, the keys aren’t only the (rowid, colid) but also contain the &lt;code&gt;TrueTime&lt;/code&gt; timestamp to provide the external consistency. At many places throughout the presentation I focused on the key differences between Spanner and BigTable, and I think they’re important to note.&lt;/p&gt;

&lt;p&gt;While BigTable motivated many columnar data stores like Cassandra and HBase; users have had a difficult time switching over to the data model exposed by it. Instead, Spanner is a drop in replacement for MySQL - something other NoSQL databases cannot claim. Although the consistency model is not eventually consistent (or shall we say, highly available, if we’re discussing CAP) because it uses Paxos to synchronize at write, it does provide speed and flexibility with the external consistency model. It will be interesting to see future papers from Google and whether or not they discuss how their internal projects float from BigTable to Megastore to Spanner, etc.&lt;/p&gt;

&lt;h3 id=&quot;links&quot;&gt;Links&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/osdi12/technical-sessions/presentation/corbett&quot;&gt;OSDI 2012 Spanner Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/BenjaminBengfort/an-overview-of-spanner-googles-globally-distributed-database&quot;&gt;CSMC 818E Presentation Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/scientific-reading/spanner/&quot;&gt;Reading &ldquo;Spanner: Google's Globally Distributed Database&rdquo;&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 28, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Reading &ldquo;The Google File System&rdquo;]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/scientific-reading/the-google-file-system/" />
  <id>http://cs.umd.edu/~bengfort/scientific-reading/the-google-file-system</id>
  <published>2014-11-21T00:00:00-05:00</published>
  <updated>2014-11-21T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;blockquote&gt;
  &lt;p&gt;&lt;small&gt;S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” in ACM SIGOPS Operating Systems Review, 2003, vol. 37, pp. 29–43.&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;While sharing many of the same goals as previous distributed file systems, our design has been driven by obser- vations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;critical-reading&quot;&gt;Critical Reading&lt;/h2&gt;

&lt;p&gt;GFS is an application-optimized distributed file system that was designed with Google’s requirements in mind. In particular, Google needed a replicated, highly-available system that prioritized streaming reads over random access and block writes or appends over random writes or overwrites. This makes perfect sense - Google’s production systems are storing entire websites as a single file, which are written as the site is being crawled (in big blocks or via appends). Computing page rank or doing other tasks requires the extraction of links or parsing of an XML file, which uses streaming reads to fetch the data. Logging and other crucial disk bound operations at Google are also append-oriented.&lt;/p&gt;

&lt;p&gt;The motto of Google’s solution seems to be &lt;em&gt;simplicity&lt;/em&gt; - and for this reason a centralized master server is used to coordinate with chunk servers that reside on many commodity machines. The Master server only handles file meta data and acts as a traffic controller to clients who want data; it does not stand pose a bottle neck to data transfer. The chunk servers themselves are simply a software layer on top of the file system of the machine; meaning that data eventually will be passed to whatever filesystem exists under GFS (and Google did report some problems with the Linux file system particularly performance woes from &lt;code&gt;fsync&lt;/code&gt; and &lt;code&gt;mmap&lt;/code&gt;). Checksums for data integrity, chunkserver selection, garbage collection, recovery, heartbeats - all of the design choices that Google made in GFS is to simplify the distributed system for the types of applications that Google uses most often.&lt;/p&gt;

&lt;p&gt;The result of this simplicity is the key innovation of this paper: the ability to use commodity hardware to provide production level distributed systems. If disks and machines are expected to fail, it is far better (and cheaper) to replace them with economically viable machines - allowing for &lt;em&gt;horizontal&lt;/em&gt; scaling; the more machines, the more capacity. There is no need for RAID or other expensive hardware because the filesystem replicates. And, extremely importantly - the data is where you will do the computation in the cluster, thus moving data off the disk to where the computation happens doesn’t require network overhead.&lt;/p&gt;

&lt;p&gt;It turns out that modern data appliances, especially those with terabytes of data, benefit from this distributed data storage model; especially when there is a distributed programming framework that also optimizes the storage model. Combined with another paper - &lt;em&gt;MapReduce: Simplified Data Processing on Large Clusters&lt;/em&gt; - GFS became the foundation for Big Data as we know it; and this paper was eventually implemented as HDFS in Hadoop. Chunks in the GFS are perfect inputs to Mapping processes, as each mapper can be run on every node in the cluster. Functional Mappers take a list of inputs as an argument and apply a function to every input value. In append-optimized systems chunks of data are therefore lists of inputs.&lt;/p&gt;

&lt;p&gt;To the critique: there is a clear bottleneck in GFS, the Master server. This server has to be smart not only about chunk allocation, but also has to handle heartbeats, read and write requests and store metadata in memory. Although it does checkpoint itself to disk and have read-only shadow backups it is a central point of failure for the cluster. Worse, the master cannot handle many small files - it is optimized for files that are 64 MB or larger. Storing the meta data for billions of small files would eat up the memory on the server which can only vertically scale. Additionally because of the 64 MB chunks, there will be some storage loss for files that do not completely use up the chunk.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/scientific-reading/the-google-file-system/&quot;&gt;Reading &ldquo;The Google File System&rdquo;&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 21, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Scientific Reading]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/thoughts/scientific-reading/" />
  <id>http://cs.umd.edu/~bengfort/thoughts/scientific-reading</id>
  <published>2014-11-20T00:00:00-05:00</published>
  <updated>2014-11-20T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Managing the reading of scientific papers is a challenge that most graduate students face because, let’s face it, scientific papers are tough to read. I would say that the first year of graduate school for me was an exercise not in learning how to do research or even higher order computer science principles but rather how to decipher academic papers (generally, not just in a single discipline). I’m still not sure that I’m an expert in reading these types of papers, but I am far more comfortable cracking open a paper and reading it than I was before; and importantly, I’m much faster at reading these papers than when I started.&lt;/p&gt;

&lt;p&gt;Reading is the first step; comprehension is the second, especially when the papers are required to directly inform what you’re thinking either for simple research papers, for coursework, or so that you can come up with new ideas. When I learned to read, I was trained in simple exercises in reading comprehension (read a passage, respond to questions). Unfortunately, narrative reading comprehension is not the same as scientific comprehension because critical evaluations of academic papers only partially use fact based extraction.&lt;/p&gt;

&lt;p&gt;Consider if you were reading &lt;em&gt;Philosophiæ Naturalis Principia Mathematica&lt;/em&gt; (one of the first scientific papers although only cited 1,750 times according to &lt;a href=&quot;http://scholar.google.com/scholar?cites=14485044281113753117&amp;amp;as_sdt=20000005&amp;amp;sciodt=0,21&amp;amp;hl=en&quot;&gt;Google Scholar&lt;/a&gt;), comprehension questions such as “What is the inverse square law?” only reveal partial understanding of the text. Better questions would allow you to elucidate why the &lt;em&gt;Principia&lt;/em&gt; is important, what work it evolved from, and what future work is required as a basis. For example, it’s important because it demonstrates a new mathematical technique that we call calculus, and uses the calculus methodology to demonstrate a precise method for computing elliptical motion around the sun, thereby proving the existence of a gravitational force. A heliocentric model of the solar system, as well as work in optics, and the discovery of the inverse square law were crucial prior work. Finally, important future work involves discovering the medium through which gravity is transferred and why celestial bodies at great distances experience the instantaneous effects of gravity.&lt;/p&gt;

&lt;p&gt;Frankly, that level of comprehension is provided to me through the benefit of nearly 400 years of reading comprehension with apologetic discourse. You don’t get nearly the same thing with papers written in the last 7 years, reviewed most likely by graduate students who weren’t willing or able to give a high level of scrutiny during the peer review process. You might also think that in computer science there would be software to review and execute; but this is also simply not true. Very often code and data are not provided to repeat experiments in a meaningful way. We are left requiring a mechanism to critically evaluate text.&lt;/p&gt;

&lt;p&gt;This semester I’m taking a class with &lt;a href=&quot;http://kelehers.me/pete/&quot;&gt;Pete Keleher&lt;/a&gt;, and as part of the course we are reading papers related to the subject matter (distributed storage systems). Dr. Keleher is having us write blog posts on every paper as part of our course participation grade, and in these blog posts, we are asked to comment on or discuss the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the problem that the paper is attempting to address?&lt;/li&gt;
  &lt;li&gt;What is innovative about this paper, or what is the contribution?&lt;/li&gt;
  &lt;li&gt;Critically evaluate the paper with both positive and negative comments.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Over the course of the semester, I’ve discovered that this simple methodology of writing 500 words or so commenting on the paper in that fashion has led to a better reading comprehension on my part. Not only that, I’ve become faster at reading papers, because now I’m reading to answer these specific questions, but not only answer the questions - but comment on them and write about it in a coherent way. I’m finding that I’m doing a first pass read of the paper at a deeper than skimming level to get a feel for the organization, the topics, and any critical knowledge required. After that, I do a Google search for the topic, or review the bibliography for any linked knowledge I can add (which was tough when I first started, but now I’m able to recognize links to common systems). Finally- I start to write, and as I write, I dive back into the paper to read or understand critical concepts that are part of my elucidation of the three points above.&lt;/p&gt;

&lt;p&gt;Needless to say, this seems like an extremely good habit to continue with as much reading as I can possibly get away with; and this blog seems like a perfect forum to publish my thoughts on other papers. And I’m hoping to make this a regular habit here. So stay tuned for more.&lt;/p&gt;

&lt;p&gt;This methodology is addressing another problem  I have as well: organization. Currently, I use &lt;a href=&quot;https://www.zotero.org/&quot;&gt;Zotero&lt;/a&gt; and &lt;a href=&quot;http://www.papershipapp.com/&quot;&gt;Papership&lt;/a&gt; to organize my research and reading. Once I get into a paper, I create a “reading list” of citations along with their abstracts; then some notes about their importance to the work I’m doing. Using the “blog post reading” methodology, I will have all of the pieces prior to the paper writing, pre-organized for my benefit. I believe that this process will be effective to making me a better scientist, and a better writer; and will update you as I go on! The first paper that I will experiment on will be a paper on diversity in Computer Science that has been sent around the department. It seems like an excellent starting place before getting into papers that are related to my research!&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/thoughts/scientific-reading/&quot;&gt;Scientific Reading&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 20, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Simple CSV Data Wrangling with Python]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/tutorials/simple-csv-data-wrangling-with-python/" />
  <id>http://cs.umd.edu/~bengfort/tutorials/simple-csv-data-wrangling-with-python</id>
  <published>2014-11-08T00:00:00-05:00</published>
  <updated>2014-11-08T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;I wanted to write a quick post today about a task that most of us do routinely but often think very little about - loading CSV (comma-separated value) data into Python. This simple action has a variety of obstacles that need to be overcome due to the nature of serialization and data transfer. In fact, I’m routinely surprised how often I have to jump through hoops to deal with this type of data, when it feels like it should be as easy as JSON or other serialization formats.&lt;/p&gt;

&lt;p&gt;The basic problem is this: CSVs have inherent schemas. In fact, most of the CSVs that I work with are dumps from a database. While the database can maintain schema information alongside the data, the scheme is lost when serializing to disk. Worse, if the dump is denormalized (a join of two tables), then the relationships are also lost, making it harder to extract entities. Although a header row can give us the names of the fields in the file, it won’t give us the type, and there is nothing structural about the serialization format (like there is with JSON) that we can infer the type from.&lt;/p&gt;

&lt;p&gt;That said, I love CSVs. CSVs are a compact data format - one row, one record. CSVs can be grown to massive sizes without cause for concern. I don’t flinch when reading 4 GB CSV files with Python because they can be split into multiple files, read one row at a time for memory efficiency, and multiprocessed with seeks to speed up the job. This is in stark contrast to JSON or XML, which have to be read completely from end to end in order to get the full data (JSON has to be completely loaded into memory and with XML you have to use a streaming parser like SAX).&lt;/p&gt;

&lt;p&gt;CSVs are the file format of choice for big data appliances like Hadoop for good reason. If you can get past encoding issues, extra dependencies, schema inference, and typing; CSVs are a great serialization format. In this post, I will provide you with a series of pro tips that I have discovered for using and wrangling CSV data.&lt;/p&gt;

&lt;p&gt;Specifically, this post will cover the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The basics of CSV processing with Python&lt;/li&gt;
  &lt;li&gt;Avoiding Unicode issues in Python 2.7&lt;/li&gt;
  &lt;li&gt;Using namedtuples or slots for memory efficiency&lt;/li&gt;
  &lt;li&gt;Serializing data with a schema using Avro&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although we won’t cover it in this post, using these techniques you have a great start towards multiprocessing to quickly dig through a CSV file from many different positions in it at once. Hopefully this intro has made CSV sound more exciting, and so let’s dive in.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/simple-csv-data-wrangling-with-python&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/tutorials/simple-csv-data-wrangling-with-python/&quot;&gt;Simple CSV Data Wrangling with Python&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 08, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Computing a Bayesian Estimate of Star Rating Means]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/tutorials/computing-a-bayesian-estimate-of-star-rating-means/" />
  <id>http://cs.umd.edu/~bengfort/tutorials/computing-a-bayesian-estimate-of-star-rating-means</id>
  <published>2014-09-11T00:00:00-04:00</published>
  <updated>2014-09-11T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Consumers rely on the collective intelligence of other consumers to protect themselves from coffee pots that break at the first sign of water, eating bad food at the wrong restaurant, and stunning flops at the theater. Although occasionally there are metrics like Rotten Tomatoes, we primarily prejudge products we would like to consume through a simple 5 star rating. This methodology is powerful, because not only does it provide a simple, easily understandable metric, but people are generally willing to reliably cast a vote by clicking a star rating without too much angst.&lt;/p&gt;

&lt;p&gt;In aggregate, this is wonderful. Star ratings can minimize individual preference because the scale is not large enough to be too nuanced. After enough ratings, it becomes pretty clear whether or not something is a hit or a flop. The problem is, however, that many ratings are needed to make this system work because the quality of a 5 star rating depends not only on the average number of stars but also on the number of reviews.&lt;/p&gt;

&lt;p&gt;Consider the following two items:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Presto Coffee Pot - average rating of 5 (1 review).&lt;/li&gt;
  &lt;li&gt;Cuisinart Brew Central - average rating of 4.1 (78 reviews).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Which item should be listed first in a list of items sorted by rating? It would seem clear that the Brew Central has more reviewers and that many of them are happy with the product. Therefore, it should probably appear before the Presto even though its average rating is lower.&lt;/p&gt;

&lt;p&gt;We could brainstorm a couple of strategies that would give us this outcome and would help us deal with items that have an insufficient number of ratings when we come across them in the future:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We could simply list the number of reviews in addition to the average rating. However, this doesn’t help with sorting the products.&lt;/li&gt;
  &lt;li&gt;We could set a floor of n number of ratings before a product is shown, but this decreases the likelihood that a product will be reviewed because it remains unseen. Not only that, but how do we choose n?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Instead, we need some empirical metric for the average rating that embeds the number of reviewers into the score. Most methodologies in use today use Bayesian estimation, which takes into account the fact that we don’t have enough data to make an estimation via the mean and also incorporates all the data we have about other observations.&lt;/p&gt;

&lt;p&gt;In this post, we will consider how to implement a Bayesian estimation of the mean of star reviews with a limited number of observations. The estimation is useful for recommender services and other predictive algorithms that use preference space measures like star reviews. It is also a good crash course in Bayesian estimation in general for those that are interested.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/computing-a-bayesian-estimate-of-star-rating-means&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/tutorials/computing-a-bayesian-estimate-of-star-rating-means/&quot;&gt;Computing a Bayesian Estimate of Star Rating Means&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on September 11, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[How to Develop Quality Python Code]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/tutorials/how-to-develop-quality-python-code/" />
  <id>http://cs.umd.edu/~bengfort/tutorials/how-to-develop-quality-python-code</id>
  <published>2014-08-20T00:00:00-04:00</published>
  <updated>2014-08-20T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Developing in Python is very different from developing in other languages. Python is an interpreted language like Ruby or Perl, so developers are able to use read-evaluate-print loops (REPLs) to execute Python in real-time. This feature of Python means that it can be used for rapid development and prototyping because there is no build process. Python includes many functional programming tools akin to Scala or Javascript to assist with closure based script development. But Python is also a fully scalable object-oriented language, a paradigm used to build large modular software rather than to simply execute scripts, more akin to Java or C++.&lt;/p&gt;

&lt;p&gt;Python sits in the middle of these paradigms, providing the best of many worlds. Python is used for writing quick one-off scripts, large scale web frameworks like Django, data processing with Celery, even numerical and scientific computing. Python is lightweight, is standard on many operating systems, and is effective, thereby making it the top choice for data scientists and analysts for data engineering and analytical tasks.&lt;/p&gt;

&lt;p&gt;However, the breadth of Python means that there is no one workflow to developing with it, and certainly there is no standard IDE or environment framework to make these decisions on your behalf. Most Python educational materials focus on the scripting aspects of the language, leaving out the important details of how to construct larger projects. This post will focus on the question of how a developer interacts with Python to build larger data applications.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/how-to-develop-quality-python-code&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/tutorials/how-to-develop-quality-python-code/&quot;&gt;How to Develop Quality Python Code&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on August 20, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Entity Resolution for Big Data]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data/" />
  <id>http://cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data</id>
  <updated>2014-03-10 09:57:07 -0400T00:00:00-00:00</updated>
  <published>2014-03-10T00:00:00-04:00</published>
  
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Entity Resolution is becoming an important discipline in Computer Science
and in Big Data especially with the recent release of Google’s
&lt;a href=&quot;http://www.google.com/insidesearch/features/search/knowledge.html&quot; title=&quot;Google's Knowledge Graph&quot;&gt;Knowledge Graph&lt;/a&gt; and the open &lt;a href=&quot;http://www.freebase.com/&quot; title=&quot;The Freebase API&quot;&gt;Freebase API&lt;/a&gt;.
Therefore it is exceptionally timely that last year at KDD 2013, Dr. Lise
Getoor of the University of Maryland and Dr. Ashwin Machanavajjhala of
Duke University gave a tutorial on
&lt;a href=&quot;http://www.kdd.org/kdd2013/accepted-tutorials&quot; title=&quot;Accepted Tutorials at KDD 2013&quot;&gt;&lt;em&gt;Entity Resolution for Big Data&lt;/em&gt;&lt;/a&gt;. We were fortunate enough
to be invited to attend a run through workshop at the Center for Scientific
Computation and Mathematical Modeling at College Park, and wanted to
highlight some of the key points for those unable to attend. This post has
been reposted from other sources.&lt;/p&gt;

&lt;style&gt;
    article img {
        margin: 28px auto;
        padding: 0;
        text-align: center;
    }
    a.image-popup {
        width: 100%;
    }
&lt;/style&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;what-is-entity-resolution&quot;&gt;What is Entity Resolution?&lt;/h2&gt;

&lt;p&gt;Entity Resolution is the task of disambiguating manifestations of real
world entities in various records or mentions by linking and grouping. For
example, there could be different ways of addressing the same person in
text, different addresses for businesses, or photos of a particular object.
This clearly has many applications, particularly in government and public
health data, web search, comparison shoppping, law enforcement, and more.&lt;/p&gt;

&lt;p&gt;Additionally, as the volume and velocity of data grows, inference across
networks and semantic relationships between entities becomes a greater
challenge. Entity Resolution can reduce the complexity by proposing
canonicalized references to particular entities and deduplicating and
linking entities. As an example, consider the following example of a
coauthor network from bibliographic data used for InfoVis 2004.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/images/network_resolution.png&quot; title=&quot;Network Resolution&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/network_resolution.png&quot; alt=&quot;ER and Network Analysis&quot; title=&quot;Network Resolution&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Deduplication significantly reduced the complexity of the network from an
ninth order graph to a much simpler fourth order graph, of significantly
less size. Dr. Getoor’s work on &lt;a href=&quot;http://www.cs.umd.edu/projects/linqs/ddupe/&quot; title=&quot;D-Dupe: Interactive Data Deduplication&quot;&gt;D-Dupe, an interactive data deduplication tool,&lt;/a&gt;
demonstrates how an iterative entity resolution process could work for
social networks, an increasingly important and growing data set that seems
to trancend traditional stovepipes like Twitter and Facebook. Yet another
example is the multiport disambiguation of Traceroute output of routers,
something that when incorrectly analyzed led to the commentary of some
technologists about the fagility of the Internet- when in fact most
Internet backbones are well defined and hardened.&lt;/p&gt;

&lt;p&gt;However, there are significant challenges to the ER discipline, least of
which is the fact that there is no unified theory and ironically, ER
itself goes by many names! Other challenges like language ambiguity, poor
data entry, missing values, changing attributes and formatting, as well as
abbreviations and truncation mean that ER is a discpline that includes not
just databases and information retrieval, but also natural language
processing and machine learning.&lt;/p&gt;

&lt;p&gt;Scaling to big data just increases the challenge, as the need for
heterogeneity and cross-domain resolution become important features. As a
result, ER techniques must be parallel and efficient, in order to be used
in the context of Big Data techniques like MapReduce and distributed graph
databases.&lt;/p&gt;

&lt;h2 id=&quot;tasks-in-entity-resolution&quot;&gt;Tasks in Entity Resolution&lt;/h2&gt;

&lt;p&gt;Generically speaking we can discuss ER as follows: there exists in the real
world entities, and in the digital world, records and mentions of those
entities. The records and mentions may take many forms, but they all refer
to only a single real world entity. We can therefore discuss the ER problem
as one involving matching record pairs corresponding to the same entity,
and as a graph of related records/mentions to related entities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/images/problem_statement.png&quot; title=&quot;Abstract ER Problem&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/problem_statement.png&quot; alt=&quot;Abstract ER Problem&quot; title=&quot;Abstract ER Problem&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: the task of clustering the records or mentions that
correspond to the same entity. There is an intensional variant of this
task: to then compute the cluster representative for each entity. This is
commonly what we think of when we consider Entity Resolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Record Linkage&lt;/strong&gt;: a slightly different version of the task is to match
records from one deduplicated data store to another. This task is proposed
in the context of already normalized data particularly in relational
databases. However, in the context of Big Data, a one to one comparison of
every record is not optimal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference Matching&lt;/strong&gt;: in this task, we must match noisy records to clean
ones in a deduplicated reference table. Note that is assumed that two
identical records are matches, this task is related to the task of entity
disambiguation.&lt;/p&gt;

&lt;p&gt;Further, when relationships between entites are added, each of the three
problem statements must also take into account the relationships between
records/mentions. Entity resolution techniques must take into account these
relationships as they are significant to disambiguation of entities.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-entity-resolution&quot;&gt;Evaluation of Entity Resolution&lt;/h3&gt;

&lt;p&gt;A quick note on the evaluation of entity resolution. For &lt;em&gt;pairwise metrics&lt;/em&gt;
we consider Precision and Recall (e.g. F1 scores), as well as the
cardinality of the number of predicted matching pairs. &lt;em&gt;Cluster level metrics&lt;/em&gt;
take into account purity, completeness, and complexity. This includes
cluster-level precision and recall, closest cluster, MUC, Rand Index, etc.
However, there has been little work on the correct prediction of links.&lt;/p&gt;

&lt;h2 id=&quot;the-algorithms-of-entity-resolution&quot;&gt;The Algorithms of Entity Resolution&lt;/h2&gt;

&lt;p&gt;This section includes a brief overview of algorithmic basis proposed by
Lise and Ashwin to provide a context for the current state of the art of
Entity Resolution. In particular, they discussed Data Preparation, Pairwise
Matching, Algoritms in Record Linkage, Deduplication, and Canonicalization.
They also considered collective entity resolution algorithms, that I will
briefly mention. Of course, they went into more depth on this section than
I will, but I hope to provide a good overview.&lt;/p&gt;

&lt;h3 id=&quot;data-preparation&quot;&gt;Data Preparation&lt;/h3&gt;

&lt;p&gt;First the tasks of &lt;em&gt;schema and data normalization&lt;/em&gt; are required preparation
for any Entity Resolution Algorithms. In this task, schema attributes are
matched (e.g. contact number vs phone number), and compound attributes like
addresses are normalized. Data normalization involves converting all
strings to upper or lower case and removing whitespace. Data cleaning and
dictionary lookups are also important.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Initial data prep is a big part of the work; smart normalization can go
a long way!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal is to construct, for a pair of records, a “comparison vector” of
&lt;em&gt;similarity scores&lt;/em&gt; of each component attribute. Similarity scores can
simply be Boolean (match or non-match) or they can be real values with
distance functions. For example, Edit distance on textual attributes can
handle typographic errors. Jaccard coefficients and other distance metrics
can be used to compare sets. Even phonetic similarity can be used.&lt;/p&gt;

&lt;h3 id=&quot;pairwise-matching&quot;&gt;Pairwise Matching&lt;/h3&gt;

&lt;p&gt;After we have constructed a vector of component-wise similarities for a
pair of records, we must compute the probability that the pair of records
is a match. There are several methods for discovering the probability of a
match. Two simple proposals are to use a weighted sum or average of
component similarity scores, and use thresholds. However, it is extremely
hard to pick weights or tune thresholds. Another simple approach can be
rule based matching, but manual formulation of rule sets is difficult.&lt;/p&gt;

&lt;p&gt;One interesting technique is the Fellegi &amp;amp; Sunter Model: given a record
pair, &lt;code&gt;r = (x,y)&lt;/code&gt; the comparison vector, is γ. If &lt;code&gt;M&lt;/code&gt; is the set of
matching pairs of records and &lt;code&gt;U&lt;/code&gt; is the set of non-matching pairs of
records, linkage decisions are based on the probability of γ given &lt;code&gt;r&lt;/code&gt;
∋ &lt;code&gt;M&lt;/code&gt; divided by the probability of γ given &lt;code&gt;r&lt;/code&gt; ∋ &lt;code&gt;U&lt;/code&gt;.
Further, you can decide if a record is a match or not based on error bounds,
μ and λ that create thresholds for whether a record is a match,
a non-match, or it is simply uncertain.&lt;/p&gt;

&lt;p&gt;In practice, Fellegi &amp;amp; Sunter requires some knowledge of matches to
train the error bounds, therefore some form of supervised learning is
required. In general, there are severaal techniques for Machine Learning
algorithms that can be applied to ER - Decision Trees, SVNS, Ensembles of
Classifiers, Conditional Random Fields, etc. The issue is Training Set
Generation since there is an imbalance of classes: non-matches far
outnumber matches, and this can result in misclassification.&lt;/p&gt;

&lt;p&gt;Active Learning methods and Unsupervised/Semi-supervised techniques, are
used to attempt to circumvent the difficulties of traning set generation.
Lise and Ashwin propose Committee of Classifiers approaches and even
crowdsourcing to leverage active learning to build a training set.&lt;/p&gt;

&lt;h3 id=&quot;constraints&quot;&gt;Constraints&lt;/h3&gt;

&lt;p&gt;There are several important forms of constraints that are relevant to the
next sections, given a specific mention, Mi:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;: If M1 and M2 match, M2 and M3 match, then M1 and M3
must also match.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exclusivity&lt;/strong&gt;: If M1 matches with M2, then M3 cannot match with M2&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Functional Dependency&lt;/strong&gt;: If M1 and M2 match, then M3 and M4 must match.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For these broad classes of constraints there are positive and negative
evidences of specific constraints, e.g. there is a converse to the match
constraint for a non-match or vice-versa. You may even consider hard and
soft constraints for the constraint types, as well as extent: can the
constraint be applied globablly or locally?&lt;/p&gt;

&lt;p&gt;Based on these constraints, you can see that transitivity is the key to
deduplication. Exclusivity is the key to record linkage and functional
dependencies are used for data cleaning. Further constraints like
aggregate, subsumption, neighboord, etc. can be used in a domain specific
context.&lt;/p&gt;

&lt;h3 id=&quot;specific-algorithms-for-problem-domains&quot;&gt;Specific Algorithms for Problem Domains&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Record Linkage&lt;/strong&gt;: propagation through the exclusitivity constraint means
that the current best solution is &lt;em&gt;Weighted K-Partite Matching&lt;/em&gt;. Edges are
pairs between records from different data sets whose weights are the
pairwise match score. The general problem is NP-hard, therefore the common
optimization is to perform successive bipartite matching.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: propegation through transitivity leads to a clustering
based entity resolution. There are a variety of clustering algorithms, but
the common input is pairwise similarity graphs. Many clustering algorithms
may also require the construction of a &lt;em&gt;cluster representative&lt;/em&gt; or a
&lt;em&gt;canonical entity&lt;/em&gt;. Although heirarhcical clustering or nearest neighbor
can be used, the recommended approach is &lt;em&gt;Correlation Clustering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Correlation Clustering uses Integer Linear Programming to maximize a cost
function that places positive and negative benefits of clustering mentions
x,y together, such that the Transitive closure is satisfied. However,
solving ILP is NP-hard, therefore a number of heuristics are used to
approximate the cost function including Greedy BEST/FIRST/VOTE algorithms,
the Greedy PIVOT algorithm, and local search.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Canonicalization&lt;/strong&gt;: Selection of the mention or cluster representative
that contains the most information. This can be rule based (e.g. longest
length), or for set value attributes a UNION. Edit distance can be used
to determine a most representative centroid, or use “majority rule”. Other
approaches include the Stanford Entity Resolution Framework (blackbox).&lt;/p&gt;

&lt;h3 id=&quot;collective-approaches&quot;&gt;Collective Approaches&lt;/h3&gt;

&lt;p&gt;If the decision for cluster-membership depends on other clusters, we can
use a collective approach. Collective approaches include non-probabilisitic
approaches like similarity propegation, or probabilistic models including
generative frameworks, or simply a hybrid approach.&lt;/p&gt;

&lt;p&gt;Generative probabilistic approaches are based on directed models, where
dependecies match decisions in a generative manner. There are a variety of
approaches, notably based on LDA and Bayesian Networks. Undirected
probabilistic approaches use semantics based on Markov Networks, to the
advantage that this allows a declarative syntax based on first-order logic
to create constraints. Several suggested approaches include Conditional
Random Fields, Markov Logic Networks (MLNs), and Probabilistic Soft Logic.&lt;/p&gt;

&lt;p&gt;Probabilistic soft logic introduces &lt;em&gt;reverse predicate equivalence&lt;/em&gt; meaning
that the same relation with the same entity gives evidence of two entities
being the same. This is not true logically, but can allow us to predict
the probability of a match with truth values in [0,1]. The declarative
language is used to define a constrained continuous Markov random field
in first order logic, and we can use relaxed logical operators. This has
significant implication for scaling improvements.&lt;/p&gt;

&lt;h2 id=&quot;scaling-to-big-data&quot;&gt;Scaling to Big Data&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Blocking/Canopy Generation&lt;/strong&gt;: The first approach to scaling to big data
is to use a blocking approach. Consider a Naïve pariwise comparsion
with 1,000 business mentions from 1,000 citties. This is &lt;em&gt;1 trillion&lt;/em&gt;
comparisons, which is 11.6 days for a microsecond comparison! However, we
know that business mentions are probably city-specific, therefore only
comparing mentions within cities reduces the comparison space to the more
managable &lt;em&gt;1 billion&lt;/em&gt; comparisions, which takes only 16 minutes at the same
rate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/images/blocking.png&quot; title=&quot;Blocking Heuristic&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/blocking.png&quot; alt=&quot;Blocking Heuristic&quot; title=&quot;Blocking Heuristic&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Although some matches can be missed using blocking techniques, the key is
to select a blocking algorithm that minimizes the subtraction of the set
of matching pair records and those satisfying the blocking criterion as in
the Venn diagram above, and maximizes the intersection. Common blocking
algorithms include hash based blocking, similarity or neighborhood based
blocking, or creating complex blocking predicates by combining simple
blocking predicates. Another powerful technique is minHash or locality
sensitive hashing, which uses distance measures and performs very well, and
is recommended as the state of the art for blocking.&lt;/p&gt;

&lt;p&gt;The final approach to blocking is more inline with the clustering methods
of earlier approaches. In &lt;em&gt;Canopy Clustering&lt;/em&gt; a distance metric is selected
with two thresholds. By picking a random mention, we create a canopy using
the first threshold and we remove all mentions where the distance from the
centroid is less than the second threshold. We continue this process so
long as the set M is not empty. This approach is interesting because
mentions can be included in more than one canopy, and therefore reduces the
chance that the blocking method excludes an actual match.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/images/canopy.png&quot; title=&quot;Canopy Clustering&quot;&gt;&lt;img src=&quot;http://cs.umd.edu/~bengfort/images/canopy.png&quot; alt=&quot;Canopy Clustering&quot; title=&quot;Canopy Clustering&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributed ER&lt;/strong&gt;: The MapReduce framework is very popular for large,
parallel tasks and there are several open source frameworks related to
Hadoop to apply to your application. MapReduce can be used with disjoint
blocking- e.g. in the Map Phase (per record computation) you compute the
blocks and associate with various reducers, and in the reduce phase (global
computation) you can perform the pairwise matching. Several other challenges
relating to implementations with MapReduce are discussed in the text and
can be reviewed during implementations of ER in MapReduce.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Entity resolution is becoming an increasingly important task as linked data
grows, and the requirement for graph based reasoning extends beyond
theoritical applications. With the advent of big data computations, this
need has become even more prevalent. Much research has been done in the
area in seperately named fields, and the tutorial by Dr. Getoor and Dr.
Machanavajjhala succinctly highlight the current state of the art and gives
a general sense of future work.&lt;/p&gt;

&lt;p&gt;Specifically, there needs to be a unified approach to the theory which can
give relational learning bounds. Since Entity Resolution is often part of
bigger inference applications, there needs to be a joint approach to
information extraction, and a characterization of how the success (or
errors) affects larger reasoning quality. Similarly, there is a need for
large, real-world datasets with ground truth to establish benchmarks for
performance.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;This has been a summary of the work done by Dr. Lise Getoor and Dr. Ashwin
Machanavajjhala for their Tutorial entitled &lt;em&gt;Entity Resolution for Big Data&lt;/em&gt;,
accepted at KDD 2013 in Chicago, Il.&lt;/p&gt;

&lt;p&gt;You can find the complete tutorial slides at:
&lt;a href=&quot;http://www.cs.umd.edu/~getoor/Tutorials/ER_KDD2013.pdf&quot; title=&quot;Tutorial Slides&quot;&gt;http://www.cs.umd.edu/~getoor/Tutorials/ER_KDD2013.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Special thanks to Lise and Ashwin for hosting Cobrain during their
run-through of the presentation.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data/&quot;&gt;Entity Resolution for Big Data&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on March 10, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Goal Driven Mixed-Initiative Systems with Collaborative Filtering]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering/" />
  <id>http://cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering</id>
  <updated>2014-02-27 10:32:10 -0500T00:00:00-00:00</updated>
  <published>2014-02-27T00:00:00-05:00</published>
  
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Mixed-initiative systems combine the efforts of human actors with artificial agents to improve the performance of both participants in some mutual functionality. For example, an intelligent search application can be viewed as a mixed-initiative system where a human uses token strings or a special query language to enact searches that are carried out by an ensemble of statistical models which deliver intelligent results. In return, they benefit from the positive feedback of the user, e.g. which links are clicked, or how many queries it takes to find a result.&lt;/p&gt;

&lt;p&gt;A mixed-initiative system is said to be goal-driven when predefined goals are used to affect the behavior of both the human user and the underlying agent. In our search example, we could talk about an “academic goal” where the user is specifically looking for research materials. In this case, the types of queries made by the user will be influenced by this goal, and the results or domain of the agent should also take this goal into account. Additionally, the agent should seek to recover from failures or anomalies by generating its own goals. By using goals, we can effectively communicate bidirectional intent as well as explain failure in mixed-initiative systems.&lt;/p&gt;

&lt;p&gt;A third, interesting dimension comes into play when there are many human users participating in a mixed-initiative system. Namely, user behavior coupled with their goals creates a latent semantic network which can be accessed to improve mixed-initiative interactions through recommendation algorithms. In the next sections, I will describe the novel application of collaborative filtering algorithms improved with goal-dimensionality to mixed-initiative systems to both allow agents to provide other human help to a human user (goal combination) as well to elicit or change the goals of a human user in real time to improve their efficacy (goal generation).&lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering-for-goal-combination&quot;&gt;Collaborative Filtering for Goal Combination&lt;/h2&gt;

&lt;p&gt;Consider an opening scene from the 1970s TV version of Mission Impossible: Peter Graves pours over the dossiers of IMF agents as the camera hones in on their skills: weapons-expert, master of disguise, a navy diver. He carefully selects the perfect team for his mission as the burnt remnants of the mission tape smolder on the table. He knows that he will only succeed in his mission if he combines the expertise of his agent in a specific mission-oriented configuration.&lt;/p&gt;

&lt;p&gt;So too can our goal driven, mixed-initiative systems assist human users on their efforts, particularly in multi-domain systems. Each human user (or even possibly agent or model) has an expertise that is expressed through the goals of each user. A financial analyst will have goals related to their particular expertise, just like a language expert might also have goals related to a particular region. However, because a human user has many, changing goals- we can build a goal-to-goal similarity matrix using the goal-vector of the user; this matrix is the basis of item-centric collaborative filtering algorithms.&lt;/p&gt;

&lt;p&gt;The goal similarity matrix will produce clusters of related goals. The system can then use the goal cluster to discover other human analysts to recommend to the original user either in a passive context (provide hints based on the behavior of the other similar users) or in an active context- directly connecting the recommended user to the original user. Team selection can also be made on this basis- selection of members from related clusters to improve the overall effectiveness of the human team. Because recommended users will have different goals than the original user, this activity can be seen as goal combination. Either way, the human user will experience highly personalized guidance on part of the system through these recommendations.&lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering-for-goal-generation&quot;&gt;Collaborative Filtering for Goal Generation&lt;/h2&gt;

&lt;p&gt;As mentioned in the previous section, a human analyst (or agent’s) goals change over time- either as (1) the result of a new mission, (2) a change in world knowledge or the environment, or (3) in order to explain or recover from failure. It would be useful for a mixed-initiative system to anticipate these changes in goals particularly in order to adapt to (2) or (3). This will lead to a much smoother transition between goals, as well as improve the efficacy of a system that has to respond to a dynamic environment.&lt;/p&gt;

&lt;p&gt;Changing the goals of the analyst in real-time is possible through the novel use of our goal-based collaborative filtering with an additional dimension- the changing state of the behavior of the user. Our similarity matrix is now not only goal-to-goal, but instead goal-to-goal-to-state. This similarity matrix can then be decomposed to a series of class based goal-to-state similarity matrices. As the state of the user changes (e.g. the nature or classification of the behavior in terms of nearest-similar goals) these more similar goals are presented to the user in order to influence the user’s behavior towards more fruitful behavior (or to allow the user to avoid potential pitfalls when they recognize an ineffective goal).&lt;/p&gt;

&lt;p&gt;In this manner, the system can affect the behavior of the user before the user is aware of the need for a change in goals. This results in a highly personalized system that is bidirectional. Not only can the system use the behavior state to recommend goals and influence human behavior, the system can also use the goals to modify the results based on positive feedback from other users with similar goals and similar behavior. For instance, the selection of a recommended goal is a positive feedback mechanism that can be used in active learning when computing goal-state similarity matrices.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering/&quot;&gt;Goal Driven Mixed-Initiative Systems with Collaborative Filtering&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 27, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Getting Moved In]]></title>
  <link rel="alternate" type="text/html" href="http://cs.umd.edu/~bengfort/news/moving-in/" />
  <id>http://cs.umd.edu/~bengfort/news/moving-in</id>
  <published>2014-01-16T14:59:24-05:00</published>
  <updated>2014-01-16T14:59:24-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Hi, I’m Ben. Today, for the fourth time, I’m starting over on my PhD. I’ve
moved into my office in A.V. Williams, and I’m feeling like a veteran
entering an extended tour. But I’m here, I’m ready to work, so let’s get
researching!&lt;/p&gt;

&lt;p&gt;Also, I do Python.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#=&amp;gt; prints &amp;#39;[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&amp;#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I build this site with &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;, which builds the static site from
Markdown in a repository.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://cs.umd.edu/~bengfort/news/moving-in/&quot;&gt;Getting Moved In&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on January 16, 2014.&lt;/p&gt;</content>
</entry>

</feed>
