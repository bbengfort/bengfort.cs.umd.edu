<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Benjamin Bengfort</title>
<subtitle type="text">A graduate research assistant in Computer Science at UMD</subtitle>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://www.cs.umd.edu/~bengfort/feed.xml" />
<link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort" />
<updated>2015-03-03T12:49:08-05:00</updated>
<id>http://www.cs.umd.edu/~bengfort/</id>
<author>
  <name>Benjamin Bengfort</name>
  <uri>http://www.cs.umd.edu/~bengfort/</uri>
  <email>bengfort@cs.umd.edu</email>
</author>


<entry>
  <title type="html"><![CDATA[Baxter Robots are Learning to Think, See, and Cook]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/thoughts/baxter-robots-are-learning-to-think-see-and-read/" />
  <id>http://www.cs.umd.edu/~bengfort/thoughts/baxter-robots-are-learning-to-think-see-and-read</id>
  <published>2015-02-26T00:00:00-05:00</published>
  <updated>2015-02-26T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;In April 2014, researchers at the University of Maryland acquired two Baxter Intera 3 humanoid robots manufactured by Rethink Robotics.  Baxter robots are well known for their use in micro-manufacturing as they are specially designed for repeated grasping and moving tasks in a stationary position.  While a Baxter’s multi-dimensional range of motion is important, What makes them especially interesting is their adaptability. Baxter robots are “trained” – not programmed – on work sites.  Human workers move the Baxter’s incredibly flexible robotic arms to “show” Baxter how to do a particular task. This ability, combined with Baxter’s powerful onboard processing, a suite of sensors, and smooth operating mechanisms makes Baxter ideal for research.&lt;/p&gt;

&lt;p&gt;In Computer Science, research groups have already begun to explore computer vision, machine learning, and artificial intelligence within a robotic context. Three research groups — the Computer Vision Labratory (CFAR), the Metacognitive Lab (MCL), and the Maryland Robotics Center — have been cooperating to produce practical results from their individual theoretical fields.  Simple tasks like picking up utensils and stirring bowls, pouring water in a moving container, and building structures from blocks have been quickly achieved thanks to this collaboration.  By studying the difficulties involved in teaching Baxter to perform these tasks, the research groups hope to solve larger theoretical challenges.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Read the full article&lt;/strong&gt;: &lt;a href=&quot;http://www.cs.umd.edu/~bengfort/papers/baxter-robots-learning.pdf&quot;&gt;Baxter Robots are Learning to Think, See, and Cook&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/thoughts/baxter-robots-are-learning-to-think-see-and-read/&quot;&gt;Baxter Robots are Learning to Think, See, and Cook&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 26, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Getting Started with Spark (in Python)]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/tutorials/getting-started-with-spark/" />
  <id>http://www.cs.umd.edu/~bengfort/tutorials/getting-started-with-spark</id>
  <published>2015-02-02T00:00:00-05:00</published>
  <updated>2015-02-02T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Hadoop is the standard tool for distributed computing across really large data sets and is the reason why you see “Big Data” on advertisements as you walk through the airport. It has become an operating system for Big Data, providing a rich ecosystem of tools and techniques that allow you to use a large cluster of relatively cheap commodity hardware to do computing at supercomputer scale. Two ideas from Google in 2003 and 2004 made Hadoop possible: a framework for distributed storage (The Google File System), which is implemented as HDFS in Hadoop, and a framework for distributed computing (MapReduce).&lt;/p&gt;

&lt;p&gt;These two ideas have been the prime drivers for the advent of scaling analytics, large scale machine learning, and other big data appliances for the last ten years! However, in technology terms, ten years is an incredibly long time, and there are some well-known limitations that exist, with MapReduce in particular. Notably, programming MapReduce is difficult. You have to chain Map and Reduce tasks together in multiple steps for most analytics. This has resulted in specialized systems for performing SQL-like computations or machine learning. Worse, MapReduce requires data to be serialized to disk between each step, which means that the I/O cost of a MapReduce job is high, making interactive analysis and iterative algorithms very expensive; and the thing is, almost all optimization and machine learning is iterative.&lt;/p&gt;

&lt;p&gt;To address these problems, Hadoop has been moving to a more general resource management framework for computation, YARN (Yet Another Resource Negotiator). YARN implements the next generation of MapReduce, but also allows applications to leverage distributed resources without having to compute with MapReduce. By generalizing the management of the cluster, research has moved toward generalizations of distributed computation, expanding the ideas first imagined in MapReduce.&lt;/p&gt;

&lt;p&gt;Spark is the first fast, general purpose distributed computing paradigm resulting from this shift and is gaining popularity rapidly. Spark extends the MapReduce model to support more types of computations using a functional programming paradigm, and it can cover a wide range of workflows that previously were implemented as specialized systems built on top of Hadoop. Spark uses in-memory caching to improve performance and, therefore, is fast enough to allow for interactive analysis (as though you were sitting on the Python interpreter, interacting with the cluster). Caching also improves the performance of iterative algorithms, which makes it great for data theoretic tasks, especially machine learning.&lt;/p&gt;

&lt;p&gt;In this post we will first discuss how to set up Spark to start easily performing analytics, either simply on your local machine or in a cluster on EC2. We then will explore Spark at an introductory level, moving towards an understanding of what Spark is and how it works (hopefully motivating further exploration). In the last two sections we will start to interact with Spark on the command line and then demo how to write a Spark application in Python and submit it to the cluster as a Spark job.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/tutorials/getting-started-with-spark/&quot;&gt;Getting Started with Spark (in Python)&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 02, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Spanner: Google's Globally Distributed Database]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/scientific-reading/spanner/" />
  <id>http://www.cs.umd.edu/~bengfort/scientific-reading/spanner</id>
  <published>2014-11-28T00:00:00-05:00</published>
  <updated>2014-11-28T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;blockquote&gt;
  &lt;p&gt;J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. J. Furman, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild, and others, “Spanner: Google’s globally distributed database,” ACM Transactions on Computer Systems (TOCS), vol. 31, no. 3, p. 8, 2013.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;Spanner is Google’s scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This paper describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: non-blocking reads in the past, lock-free read-only transactions, and atomic schema changes, across all of Spanner.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;critical-reading&quot;&gt;Critical Reading&lt;/h2&gt;

&lt;p&gt;When dealing with data stores that contain mission critical or extremely valuable information that is used by many individuals all over the world, you want to ensure that the data remains consistent and is durable across data centers that span the globe. Data replication provides durability, clients accessing geographically close servers gives low latency, and because multiple replicas have data, failure can be easily handled. Creating a database system that manages cross-data center replication  that is synchronously replicated, however, is a major challenge especially when attempting to allow lock-free read only transactions and atomic, linear changes that are consistent and versioned.&lt;/p&gt;

&lt;p&gt;To date, the systems that we have discussed in class have focused on transactional invariants that give stronger semantics than simple eventual consistency to provide a data service that expands our understanding of CAP. These data systems have primarily been key-value stores, which simplifies the data storage expectations to look at transactional invariants (and explore DHTs and other techniques). However, from the application perspective, a key-value store is difficult to use for applications that have complex schemas that might be subject to change or for applications that are happy to lose some availability in favor of strong semantics for wide area consistency.&lt;/p&gt;

&lt;p&gt;Spanner is a globally distributed database that provides &lt;em&gt;external consistency&lt;/em&gt; between data centers and stores data in a schema based semi-relational data structure. Not only that, Spanner provides a versioned view of the data that allows for instantaneous snapshot isolation across any segment of the data. This versioned isolation allows Spanner to provide globally consistent reads of the database at a particular time allowing for lock-free read-only transactions (and therefore no communications overhead for consensus during these types of reads). Spanner also provides externally consistent reads and writes with a timestamp-based linear execution of transactions and two phase commits. Spanner is the first distributed database that provides global sharding and replication with strong consistency semantics.&lt;/p&gt;

&lt;p&gt;The primary innovation of Spanner is its implementation of timestamp management an linear serializability of transactions via a trusted global time, a system they call &lt;code&gt;TrueTime&lt;/code&gt;. &lt;code&gt;TrueTime&lt;/code&gt; represents timestamps as an interval, the timestamp +/- some uncertainty concerning the true time. The use of uncertainty means that Spanner will slow down to ensure that there is no local time conflict with the global system time and provide a reliable timestamp for each transaction in the system. The implementation of TrueTime involves the use of distributed master time servers that have GPS and atomic clocks and an polling daemon on clients that implements a time synchronization protocol with several masters and computes uncertainty of local clock drift.&lt;/p&gt;

&lt;p&gt;I was duly impressed with Spanner - not only does it provide a strong consistency model across a globally distributed system, it also implements a semi-relational interface that more applications can use with ease over NoSQL proposals (including a structured query language, schemas, and client-specific control of wide-area replication). Spanner is rightly robust (as it contains Google’s most valuable data, the advertising network) with an impressive latency, even using Paxos for consensus under the hood. However, this system is not practical as an open source, academic, or research system. Google calls a Spanner implementation a “universe” and there are only three Google Spanner “universes” and rightly so - multiple data centers with atomic and GPS clocks is an expensive proposition! However, because the innovation relies on strong universal time, and the use of Marzullo’s algorithm to ensure multiple timeservers give an accurate time stamp, it may be possible to rely on the strong global time invariant to explore other consistency models (much like GPS allows us to explore other scientific research related to things beyond navigation) so long as someone sets up a universally available &lt;code&gt;TrueTime&lt;/code&gt; protocol system that anyone can connect to!&lt;/p&gt;

&lt;h2 id=&quot;other-notes&quot;&gt;Other Notes&lt;/h2&gt;

&lt;p&gt;This paper was my responsibility for presentation to the course, and to start the presentation, I was asked to play the video of the presentation by Wilson Hsieh at OSDI 2012 (links below). Wilson focused (rightly so) on &lt;code&gt;TrueTime&lt;/code&gt; after giving a clear example of why Spanner is important in the context of a social network. He skipped most of the implementation details of Spanner that were presented in the paper. My presentation focused primarily on the implementation details, since &lt;code&gt;TrueTime&lt;/code&gt; was discussed in detail, the links to the presentation I gave can also be found below.&lt;/p&gt;

&lt;p&gt;In my presentation, I couldn’t help compare Spanner to BigTable, another Google paper that I may have to blog about (and my systems research may as well be called Google distributed systems!). Like BigTable, Spanner uses a data structure called Tablets under the hood, essentially a bag of related key value pairs. Unlike BigTable, the keys aren’t only the (rowid, colid) but also contain the &lt;code&gt;TrueTime&lt;/code&gt; timestamp to provide the external consistency. At many places throughout the presentation I focused on the key differences between Spanner and BigTable, and I think they’re important to note.&lt;/p&gt;

&lt;p&gt;While BigTable motivated many columnar data stores like Cassandra and HBase; users have had a difficult time switching over to the data model exposed by it. Instead, Spanner is a drop in replacement for MySQL - something other NoSQL databases cannot claim. Although the consistency model is not eventually consistent (or shall we say, highly available, if we’re discussing CAP) because it uses Paxos to synchronize at write, it does provide speed and flexibility with the external consistency model. It will be interesting to see future papers from Google and whether or not they discuss how their internal projects float from BigTable to Megastore to Spanner, etc.&lt;/p&gt;

&lt;h3 id=&quot;links&quot;&gt;Links&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.usenix.org/conference/osdi12/technical-sessions/presentation/corbett&quot;&gt;OSDI 2012 Spanner Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/BenjaminBengfort/an-overview-of-spanner-googles-globally-distributed-database&quot;&gt;CSMC 818E Presentation Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/scientific-reading/spanner/&quot;&gt;Spanner: Google's Globally Distributed Database&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 28, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[The Google File System]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/scientific-reading/the-google-file-system/" />
  <id>http://www.cs.umd.edu/~bengfort/scientific-reading/the-google-file-system</id>
  <published>2014-11-21T00:00:00-05:00</published>
  <updated>2014-11-21T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;blockquote&gt;
  &lt;p&gt;S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” in ACM SIGOPS Operating Systems Review, 2003, vol. 37, pp. 29–43.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;While sharing many of the same goals as previous distributed file systems, our design has been driven by obser- vations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;critical-reading&quot;&gt;Critical Reading&lt;/h2&gt;

&lt;p&gt;GFS is an application-optimized distributed file system that was designed with Google’s requirements in mind. In particular, Google needed a replicated, highly-available system that prioritized streaming reads over random access and block writes or appends over random writes or overwrites. This makes perfect sense - Google’s production systems are storing entire websites as a single file, which are written as the site is being crawled (in big blocks or via appends). Computing page rank or doing other tasks requires the extraction of links or parsing of an XML file, which uses streaming reads to fetch the data. Logging and other crucial disk bound operations at Google are also append-oriented.&lt;/p&gt;

&lt;p&gt;The motto of Google’s solution seems to be &lt;em&gt;simplicity&lt;/em&gt; - and for this reason a centralized master server is used to coordinate with chunk servers that reside on many commodity machines. The Master server only handles file meta data and acts as a traffic controller to clients who want data; it does not stand pose a bottle neck to data transfer. The chunk servers themselves are simply a software layer on top of the file system of the machine; meaning that data eventually will be passed to whatever filesystem exists under GFS (and Google did report some problems with the Linux file system particularly performance woes from &lt;code&gt;fsync&lt;/code&gt; and &lt;code&gt;mmap&lt;/code&gt;). Checksums for data integrity, chunkserver selection, garbage collection, recovery, heartbeats - all of the design choices that Google made in GFS is to simplify the distributed system for the types of applications that Google uses most often.&lt;/p&gt;

&lt;p&gt;The result of this simplicity is the key innovation of this paper: the ability to use commodity hardware to provide production level distributed systems. If disks and machines are expected to fail, it is far better (and cheaper) to replace them with economically viable machines - allowing for &lt;em&gt;horizontal&lt;/em&gt; scaling; the more machines, the more capacity. There is no need for RAID or other expensive hardware because the filesystem replicates. And, extremely importantly - the data is where you will do the computation in the cluster, thus moving data off the disk to where the computation happens doesn’t require network overhead.&lt;/p&gt;

&lt;p&gt;It turns out that modern data appliances, especially those with terabytes of data, benefit from this distributed data storage model; especially when there is a distributed programming framework that also optimizes the storage model. Combined with another paper - &lt;em&gt;MapReduce: Simplified Data Processing on Large Clusters&lt;/em&gt; - GFS became the foundation for Big Data as we know it; and this paper was eventually implemented as HDFS in Hadoop. Chunks in the GFS are perfect inputs to Mapping processes, as each mapper can be run on every node in the cluster. Functional Mappers take a list of inputs as an argument and apply a function to every input value. In append-optimized systems chunks of data are therefore lists of inputs.&lt;/p&gt;

&lt;p&gt;To the critique: there is a clear bottleneck in GFS, the Master server. This server has to be smart not only about chunk allocation, but also has to handle heartbeats, read and write requests and store metadata in memory. Although it does checkpoint itself to disk and have read-only shadow backups it is a central point of failure for the cluster. Worse, the master cannot handle many small files - it is optimized for files that are 64 MB or larger. Storing the meta data for billions of small files would eat up the memory on the server which can only vertically scale. Additionally because of the 64 MB chunks, there will be some storage loss for files that do not completely use up the chunk.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/scientific-reading/the-google-file-system/&quot;&gt;The Google File System&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 21, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Scientific Reading]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/thoughts/scientific-reading/" />
  <id>http://www.cs.umd.edu/~bengfort/thoughts/scientific-reading</id>
  <published>2014-11-20T00:00:00-05:00</published>
  <updated>2014-11-20T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Managing the reading of scientific papers is a challenge that most graduate students face because, let’s face it, scientific papers are tough to read. I would say that the first year of graduate school for me was an exercise not in learning how to do research or even higher order computer science principles but rather how to decipher academic papers (generally, not just in a single discipline). I’m still not sure that I’m an expert in reading these types of papers, but I am far more comfortable cracking open a paper and reading it than I was before; and importantly, I’m much faster at reading these papers than when I started.&lt;/p&gt;

&lt;p&gt;Reading is the first step; comprehension is the second, especially when the papers are required to directly inform what you’re thinking either for simple research papers, for coursework, or so that you can come up with new ideas. When I learned to read, I was trained in simple exercises in reading comprehension (read a passage, respond to questions). Unfortunately, narrative reading comprehension is not the same as scientific comprehension because critical evaluations of academic papers only partially use fact based extraction.&lt;/p&gt;

&lt;p&gt;Consider if you were reading &lt;em&gt;Philosophiæ Naturalis Principia Mathematica&lt;/em&gt; (one of the first scientific papers although only cited 1,750 times according to &lt;a href=&quot;http://scholar.google.com/scholar?cites=14485044281113753117&amp;amp;as_sdt=20000005&amp;amp;sciodt=0,21&amp;amp;hl=en&quot;&gt;Google Scholar&lt;/a&gt;), comprehension questions such as “What is the inverse square law?” only reveal partial understanding of the text. Better questions would allow you to elucidate why the &lt;em&gt;Principia&lt;/em&gt; is important, what work it evolved from, and what future work is required as a basis. For example, it’s important because it demonstrates a new mathematical technique that we call calculus, and uses the calculus methodology to demonstrate a precise method for computing elliptical motion around the sun, thereby proving the existence of a gravitational force. A heliocentric model of the solar system, as well as work in optics, and the discovery of the inverse square law were crucial prior work. Finally, important future work involves discovering the medium through which gravity is transferred and why celestial bodies at great distances experience the instantaneous effects of gravity.&lt;/p&gt;

&lt;p&gt;Frankly, that level of comprehension is provided to me through the benefit of nearly 400 years of reading comprehension with apologetic discourse. You don’t get nearly the same thing with papers written in the last 7 years, reviewed most likely by graduate students who weren’t willing or able to give a high level of scrutiny during the peer review process. You might also think that in computer science there would be software to review and execute; but this is also simply not true. Very often code and data are not provided to repeat experiments in a meaningful way. We are left requiring a mechanism to critically evaluate text.&lt;/p&gt;

&lt;p&gt;This semester I’m taking a class with &lt;a href=&quot;http://kelehers.me/pete/&quot;&gt;Pete Keleher&lt;/a&gt;, and as part of the course we are reading papers related to the subject matter (distributed storage systems). Dr. Keleher is having us write blog posts on every paper as part of our course participation grade, and in these blog posts, we are asked to comment on or discuss the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What is the problem that the paper is attempting to address?&lt;/li&gt;
  &lt;li&gt;What is innovative about this paper, or what is the contribution?&lt;/li&gt;
  &lt;li&gt;Critically evaluate the paper with both positive and negative comments.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Over the course of the semester, I’ve discovered that this simple methodology of writing 500 words or so commenting on the paper in that fashion has led to a better reading comprehension on my part. Not only that, I’ve become faster at reading papers, because now I’m reading to answer these specific questions, but not only answer the questions - but comment on them and write about it in a coherent way. I’m finding that I’m doing a first pass read of the paper at a deeper than skimming level to get a feel for the organization, the topics, and any critical knowledge required. After that, I do a Google search for the topic, or review the bibliography for any linked knowledge I can add (which was tough when I first started, but now I’m able to recognize links to common systems). Finally- I start to write, and as I write, I dive back into the paper to read or understand critical concepts that are part of my elucidation of the three points above.&lt;/p&gt;

&lt;p&gt;Needless to say, this seems like an extremely good habit to continue with as much reading as I can possibly get away with; and this blog seems like a perfect forum to publish my thoughts on other papers. And I’m hoping to make this a regular habit here. So stay tuned for more.&lt;/p&gt;

&lt;p&gt;This methodology is addressing another problem  I have as well: organization. Currently, I use &lt;a href=&quot;https://www.zotero.org/&quot;&gt;Zotero&lt;/a&gt; and &lt;a href=&quot;http://www.papershipapp.com/&quot;&gt;Papership&lt;/a&gt; to organize my research and reading. Once I get into a paper, I create a “reading list” of citations along with their abstracts; then some notes about their importance to the work I’m doing. Using the “blog post reading” methodology, I will have all of the pieces prior to the paper writing, pre-organized for my benefit. I believe that this process will be effective to making me a better scientist, and a better writer; and will update you as I go on! The first paper that I will experiment on will be a paper on diversity in Computer Science that has been sent around the department. It seems like an excellent starting place before getting into papers that are related to my research!&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/thoughts/scientific-reading/&quot;&gt;Scientific Reading&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 20, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Simple CSV Data Wrangling with Python]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/tutorials/simple-csv-data-wrangling-with-python/" />
  <id>http://www.cs.umd.edu/~bengfort/tutorials/simple-csv-data-wrangling-with-python</id>
  <published>2014-11-08T00:00:00-05:00</published>
  <updated>2014-11-08T00:00:00-05:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;I wanted to write a quick post today about a task that most of us do routinely but often think very little about - loading CSV (comma-separated value) data into Python. This simple action has a variety of obstacles that need to be overcome due to the nature of serialization and data transfer. In fact, I’m routinely surprised how often I have to jump through hoops to deal with this type of data, when it feels like it should be as easy as JSON or other serialization formats.&lt;/p&gt;

&lt;p&gt;The basic problem is this: CSVs have inherent schemas. In fact, most of the CSVs that I work with are dumps from a database. While the database can maintain schema information alongside the data, the scheme is lost when serializing to disk. Worse, if the dump is denormalized (a join of two tables), then the relationships are also lost, making it harder to extract entities. Although a header row can give us the names of the fields in the file, it won’t give us the type, and there is nothing structural about the serialization format (like there is with JSON) that we can infer the type from.&lt;/p&gt;

&lt;p&gt;That said, I love CSVs. CSVs are a compact data format - one row, one record. CSVs can be grown to massive sizes without cause for concern. I don’t flinch when reading 4 GB CSV files with Python because they can be split into multiple files, read one row at a time for memory efficiency, and multiprocessed with seeks to speed up the job. This is in stark contrast to JSON or XML, which have to be read completely from end to end in order to get the full data (JSON has to be completely loaded into memory and with XML you have to use a streaming parser like SAX).&lt;/p&gt;

&lt;p&gt;CSVs are the file format of choice for big data appliances like Hadoop for good reason. If you can get past encoding issues, extra dependencies, schema inference, and typing; CSVs are a great serialization format. In this post, I will provide you with a series of pro tips that I have discovered for using and wrangling CSV data.&lt;/p&gt;

&lt;p&gt;Specifically, this post will cover the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The basics of CSV processing with Python&lt;/li&gt;
  &lt;li&gt;Avoiding Unicode issues in Python 2.7&lt;/li&gt;
  &lt;li&gt;Using namedtuples or slots for memory efficiency&lt;/li&gt;
  &lt;li&gt;Serializing data with a schema using Avro&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although we won’t cover it in this post, using these techniques you have a great start towards multiprocessing to quickly dig through a CSV file from many different positions in it at once. Hopefully this intro has made CSV sound more exciting, and so let’s dive in.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/simple-csv-data-wrangling-with-python&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/tutorials/simple-csv-data-wrangling-with-python/&quot;&gt;Simple CSV Data Wrangling with Python&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on November 08, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Computing a Bayesian Estimate of Star Rating Means]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/tutorials/computing-a-bayesian-estimate-of-star-rating-means/" />
  <id>http://www.cs.umd.edu/~bengfort/tutorials/computing-a-bayesian-estimate-of-star-rating-means</id>
  <published>2014-09-11T00:00:00-04:00</published>
  <updated>2014-09-11T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Consumers rely on the collective intelligence of other consumers to protect themselves from coffee pots that break at the first sign of water, eating bad food at the wrong restaurant, and stunning flops at the theater. Although occasionally there are metrics like Rotten Tomatoes, we primarily prejudge products we would like to consume through a simple 5 star rating. This methodology is powerful, because not only does it provide a simple, easily understandable metric, but people are generally willing to reliably cast a vote by clicking a star rating without too much angst.&lt;/p&gt;

&lt;p&gt;In aggregate, this is wonderful. Star ratings can minimize individual preference because the scale is not large enough to be too nuanced. After enough ratings, it becomes pretty clear whether or not something is a hit or a flop. The problem is, however, that many ratings are needed to make this system work because the quality of a 5 star rating depends not only on the average number of stars but also on the number of reviews.&lt;/p&gt;

&lt;p&gt;Consider the following two items:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Presto Coffee Pot - average rating of 5 (1 review).&lt;/li&gt;
  &lt;li&gt;Cuisinart Brew Central - average rating of 4.1 (78 reviews).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Which item should be listed first in a list of items sorted by rating? It would seem clear that the Brew Central has more reviewers and that many of them are happy with the product. Therefore, it should probably appear before the Presto even though its average rating is lower.&lt;/p&gt;

&lt;p&gt;We could brainstorm a couple of strategies that would give us this outcome and would help us deal with items that have an insufficient number of ratings when we come across them in the future:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We could simply list the number of reviews in addition to the average rating. However, this doesn’t help with sorting the products.&lt;/li&gt;
  &lt;li&gt;We could set a floor of n number of ratings before a product is shown, but this decreases the likelihood that a product will be reviewed because it remains unseen. Not only that, but how do we choose n?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Instead, we need some empirical metric for the average rating that embeds the number of reviewers into the score. Most methodologies in use today use Bayesian estimation, which takes into account the fact that we don’t have enough data to make an estimation via the mean and also incorporates all the data we have about other observations.&lt;/p&gt;

&lt;p&gt;In this post, we will consider how to implement a Bayesian estimation of the mean of star reviews with a limited number of observations. The estimation is useful for recommender services and other predictive algorithms that use preference space measures like star reviews. It is also a good crash course in Bayesian estimation in general for those that are interested.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/computing-a-bayesian-estimate-of-star-rating-means&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/tutorials/computing-a-bayesian-estimate-of-star-rating-means/&quot;&gt;Computing a Bayesian Estimate of Star Rating Means&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on September 11, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[How to Develop Quality Python Code]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/tutorials/how-to-develop-quality-python-code/" />
  <id>http://www.cs.umd.edu/~bengfort/tutorials/how-to-develop-quality-python-code</id>
  <published>2014-08-20T00:00:00-04:00</published>
  <updated>2014-08-20T00:00:00-04:00</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Developing in Python is very different from developing in other languages. Python is an interpreted language like Ruby or Perl, so developers are able to use read-evaluate-print loops (REPLs) to execute Python in real-time. This feature of Python means that it can be used for rapid development and prototyping because there is no build process. Python includes many functional programming tools akin to Scala or Javascript to assist with closure based script development. But Python is also a fully scalable object-oriented language, a paradigm used to build large modular software rather than to simply execute scripts, more akin to Java or C++.&lt;/p&gt;

&lt;p&gt;Python sits in the middle of these paradigms, providing the best of many worlds. Python is used for writing quick one-off scripts, large scale web frameworks like Django, data processing with Celery, even numerical and scientific computing. Python is lightweight, is standard on many operating systems, and is effective, thereby making it the top choice for data scientists and analysts for data engineering and analytical tasks.&lt;/p&gt;

&lt;p&gt;However, the breadth of Python means that there is no one workflow to developing with it, and certainly there is no standard IDE or environment framework to make these decisions on your behalf. Most Python educational materials focus on the scripting aspects of the language, leaving out the important details of how to construct larger projects. This post will focus on the question of how a developer interacts with Python to build larger data applications.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://districtdatalabs.silvrback.com/how-to-develop-quality-python-code&quot;&gt;Read the complete article at District Data Labs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/tutorials/how-to-develop-quality-python-code/&quot;&gt;How to Develop Quality Python Code&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on August 20, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Entity Resolution for Big Data]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data/" />
  <id>http://www.cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data</id>
  <updated>2014-03-10 13:57:07 UTCT00:00:00-00:00</updated>
  <published>2014-03-10T00:00:00-04:00</published>
  
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Entity Resolution is becoming an important discipline in Computer Science
and in Big Data especially with the recent release of Google’s
&lt;a href=&quot;http://www.google.com/insidesearch/features/search/knowledge.html&quot; title=&quot;Google&#39;s Knowledge Graph&quot;&gt;Knowledge Graph&lt;/a&gt; and the open &lt;a href=&quot;http://www.freebase.com/&quot; title=&quot;The Freebase API&quot;&gt;Freebase API&lt;/a&gt;.
Therefore it is exceptionally timely that last year at KDD 2013, Dr. Lise
Getoor of the University of Maryland and Dr. Ashwin Machanavajjhala of
Duke University gave a tutorial on
&lt;a href=&quot;http://www.kdd.org/kdd2013/accepted-tutorials&quot; title=&quot;Accepted Tutorials at KDD 2013&quot;&gt;&lt;em&gt;Entity Resolution for Big Data&lt;/em&gt;&lt;/a&gt;. We were fortunate enough
to be invited to attend a run through workshop at the Center for Scientific
Computation and Mathematical Modeling at College Park, and wanted to
highlight some of the key points for those unable to attend. This post has
been reposted from other sources.&lt;/p&gt;

&lt;style&gt;
    article img {
        margin: 28px auto;
        padding: 0;
        text-align: center;
    }
    a.image-popup {
        width: 100%;
    }
&lt;/style&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;what-is-entity-resolution&quot;&gt;What is Entity Resolution?&lt;/h2&gt;

&lt;p&gt;Entity Resolution is the task of disambiguating manifestations of real
world entities in various records or mentions by linking and grouping. For
example, there could be different ways of addressing the same person in
text, different addresses for businesses, or photos of a particular object.
This clearly has many applications, particularly in government and public
health data, web search, comparison shoppping, law enforcement, and more.&lt;/p&gt;

&lt;p&gt;Additionally, as the volume and velocity of data grows, inference across
networks and semantic relationships between entities becomes a greater
challenge. Entity Resolution can reduce the complexity by proposing
canonicalized references to particular entities and deduplicating and
linking entities. As an example, consider the following example of a
coauthor network from bibliographic data used for InfoVis 2004.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/network_resolution.png&quot; title=&quot;Network Resolution&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/network_resolution.png&quot; alt=&quot;ER and Network Analysis&quot; title=&quot;Network Resolution&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Deduplication significantly reduced the complexity of the network from an
ninth order graph to a much simpler fourth order graph, of significantly
less size. Dr. Getoor’s work on &lt;a href=&quot;http://www.cs.umd.edu/projects/linqs/ddupe/&quot; title=&quot;D-Dupe: Interactive Data Deduplication&quot;&gt;D-Dupe, an interactive data deduplication tool,&lt;/a&gt;
demonstrates how an iterative entity resolution process could work for
social networks, an increasingly important and growing data set that seems
to trancend traditional stovepipes like Twitter and Facebook. Yet another
example is the multiport disambiguation of Traceroute output of routers,
something that when incorrectly analyzed led to the commentary of some
technologists about the fagility of the Internet- when in fact most
Internet backbones are well defined and hardened.&lt;/p&gt;

&lt;p&gt;However, there are significant challenges to the ER discipline, least of
which is the fact that there is no unified theory and ironically, ER
itself goes by many names! Other challenges like language ambiguity, poor
data entry, missing values, changing attributes and formatting, as well as
abbreviations and truncation mean that ER is a discpline that includes not
just databases and information retrieval, but also natural language
processing and machine learning.&lt;/p&gt;

&lt;p&gt;Scaling to big data just increases the challenge, as the need for
heterogeneity and cross-domain resolution become important features. As a
result, ER techniques must be parallel and efficient, in order to be used
in the context of Big Data techniques like MapReduce and distributed graph
databases.&lt;/p&gt;

&lt;h2 id=&quot;tasks-in-entity-resolution&quot;&gt;Tasks in Entity Resolution&lt;/h2&gt;

&lt;p&gt;Generically speaking we can discuss ER as follows: there exists in the real
world entities, and in the digital world, records and mentions of those
entities. The records and mentions may take many forms, but they all refer
to only a single real world entity. We can therefore discuss the ER problem
as one involving matching record pairs corresponding to the same entity,
and as a graph of related records/mentions to related entities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/problem_statement.png&quot; title=&quot;Abstract ER Problem&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/problem_statement.png&quot; alt=&quot;Abstract ER Problem&quot; title=&quot;Abstract ER Problem&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: the task of clustering the records or mentions that
correspond to the same entity. There is an intensional variant of this
task: to then compute the cluster representative for each entity. This is
commonly what we think of when we consider Entity Resolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Record Linkage&lt;/strong&gt;: a slightly different version of the task is to match
records from one deduplicated data store to another. This task is proposed
in the context of already normalized data particularly in relational
databases. However, in the context of Big Data, a one to one comparison of
every record is not optimal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference Matching&lt;/strong&gt;: in this task, we must match noisy records to clean
ones in a deduplicated reference table. Note that is assumed that two
identical records are matches, this task is related to the task of entity
disambiguation.&lt;/p&gt;

&lt;p&gt;Further, when relationships between entites are added, each of the three
problem statements must also take into account the relationships between
records/mentions. Entity resolution techniques must take into account these
relationships as they are significant to disambiguation of entities.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-entity-resolution&quot;&gt;Evaluation of Entity Resolution&lt;/h3&gt;

&lt;p&gt;A quick note on the evaluation of entity resolution. For &lt;em&gt;pairwise metrics&lt;/em&gt;
we consider Precision and Recall (e.g. F1 scores), as well as the
cardinality of the number of predicted matching pairs. &lt;em&gt;Cluster level metrics&lt;/em&gt;
take into account purity, completeness, and complexity. This includes
cluster-level precision and recall, closest cluster, MUC, Rand Index, etc.
However, there has been little work on the correct prediction of links.&lt;/p&gt;

&lt;h2 id=&quot;the-algorithms-of-entity-resolution&quot;&gt;The Algorithms of Entity Resolution&lt;/h2&gt;

&lt;p&gt;This section includes a brief overview of algorithmic basis proposed by
Lise and Ashwin to provide a context for the current state of the art of
Entity Resolution. In particular, they discussed Data Preparation, Pairwise
Matching, Algoritms in Record Linkage, Deduplication, and Canonicalization.
They also considered collective entity resolution algorithms, that I will
briefly mention. Of course, they went into more depth on this section than
I will, but I hope to provide a good overview.&lt;/p&gt;

&lt;h3 id=&quot;data-preparation&quot;&gt;Data Preparation&lt;/h3&gt;

&lt;p&gt;First the tasks of &lt;em&gt;schema and data normalization&lt;/em&gt; are required preparation
for any Entity Resolution Algorithms. In this task, schema attributes are
matched (e.g. contact number vs phone number), and compound attributes like
addresses are normalized. Data normalization involves converting all
strings to upper or lower case and removing whitespace. Data cleaning and
dictionary lookups are also important.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Initial data prep is a big part of the work; smart normalization can go
a long way!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal is to construct, for a pair of records, a “comparison vector” of
&lt;em&gt;similarity scores&lt;/em&gt; of each component attribute. Similarity scores can
simply be Boolean (match or non-match) or they can be real values with
distance functions. For example, Edit distance on textual attributes can
handle typographic errors. Jaccard coefficients and other distance metrics
can be used to compare sets. Even phonetic similarity can be used.&lt;/p&gt;

&lt;h3 id=&quot;pairwise-matching&quot;&gt;Pairwise Matching&lt;/h3&gt;

&lt;p&gt;After we have constructed a vector of component-wise similarities for a
pair of records, we must compute the probability that the pair of records
is a match. There are several methods for discovering the probability of a
match. Two simple proposals are to use a weighted sum or average of
component similarity scores, and use thresholds. However, it is extremely
hard to pick weights or tune thresholds. Another simple approach can be
rule based matching, but manual formulation of rule sets is difficult.&lt;/p&gt;

&lt;p&gt;One interesting technique is the Fellegi &amp;amp; Sunter Model: given a record
pair, &lt;code&gt;r = (x,y)&lt;/code&gt; the comparison vector, is γ. If &lt;code&gt;M&lt;/code&gt; is the set of
matching pairs of records and &lt;code&gt;U&lt;/code&gt; is the set of non-matching pairs of
records, linkage decisions are based on the probability of γ given &lt;code&gt;r&lt;/code&gt;
∋ &lt;code&gt;M&lt;/code&gt; divided by the probability of γ given &lt;code&gt;r&lt;/code&gt; ∋ &lt;code&gt;U&lt;/code&gt;.
Further, you can decide if a record is a match or not based on error bounds,
μ and λ that create thresholds for whether a record is a match,
a non-match, or it is simply uncertain.&lt;/p&gt;

&lt;p&gt;In practice, Fellegi &amp;amp; Sunter requires some knowledge of matches to
train the error bounds, therefore some form of supervised learning is
required. In general, there are severaal techniques for Machine Learning
algorithms that can be applied to ER - Decision Trees, SVNS, Ensembles of
Classifiers, Conditional Random Fields, etc. The issue is Training Set
Generation since there is an imbalance of classes: non-matches far
outnumber matches, and this can result in misclassification.&lt;/p&gt;

&lt;p&gt;Active Learning methods and Unsupervised/Semi-supervised techniques, are
used to attempt to circumvent the difficulties of traning set generation.
Lise and Ashwin propose Committee of Classifiers approaches and even
crowdsourcing to leverage active learning to build a training set.&lt;/p&gt;

&lt;h3 id=&quot;constraints&quot;&gt;Constraints&lt;/h3&gt;

&lt;p&gt;There are several important forms of constraints that are relevant to the
next sections, given a specific mention, Mi:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Transitivity&lt;/strong&gt;: If M1 and M2 match, M2 and M3 match, then M1 and M3
must also match.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exclusivity&lt;/strong&gt;: If M1 matches with M2, then M3 cannot match with M2&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Functional Dependency&lt;/strong&gt;: If M1 and M2 match, then M3 and M4 must match.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For these broad classes of constraints there are positive and negative
evidences of specific constraints, e.g. there is a converse to the match
constraint for a non-match or vice-versa. You may even consider hard and
soft constraints for the constraint types, as well as extent: can the
constraint be applied globablly or locally?&lt;/p&gt;

&lt;p&gt;Based on these constraints, you can see that transitivity is the key to
deduplication. Exclusivity is the key to record linkage and functional
dependencies are used for data cleaning. Further constraints like
aggregate, subsumption, neighboord, etc. can be used in a domain specific
context.&lt;/p&gt;

&lt;h3 id=&quot;specific-algorithms-for-problem-domains&quot;&gt;Specific Algorithms for Problem Domains&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Record Linkage&lt;/strong&gt;: propagation through the exclusitivity constraint means
that the current best solution is &lt;em&gt;Weighted K-Partite Matching&lt;/em&gt;. Edges are
pairs between records from different data sets whose weights are the
pairwise match score. The general problem is NP-hard, therefore the common
optimization is to perform successive bipartite matching.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: propegation through transitivity leads to a clustering
based entity resolution. There are a variety of clustering algorithms, but
the common input is pairwise similarity graphs. Many clustering algorithms
may also require the construction of a &lt;em&gt;cluster representative&lt;/em&gt; or a
&lt;em&gt;canonical entity&lt;/em&gt;. Although heirarhcical clustering or nearest neighbor
can be used, the recommended approach is &lt;em&gt;Correlation Clustering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Correlation Clustering uses Integer Linear Programming to maximize a cost
function that places positive and negative benefits of clustering mentions
x,y together, such that the Transitive closure is satisfied. However,
solving ILP is NP-hard, therefore a number of heuristics are used to
approximate the cost function including Greedy BEST/FIRST/VOTE algorithms,
the Greedy PIVOT algorithm, and local search.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Canonicalization&lt;/strong&gt;: Selection of the mention or cluster representative
that contains the most information. This can be rule based (e.g. longest
length), or for set value attributes a UNION. Edit distance can be used
to determine a most representative centroid, or use “majority rule”. Other
approaches include the Stanford Entity Resolution Framework (blackbox).&lt;/p&gt;

&lt;h3 id=&quot;collective-approaches&quot;&gt;Collective Approaches&lt;/h3&gt;

&lt;p&gt;If the decision for cluster-membership depends on other clusters, we can
use a collective approach. Collective approaches include non-probabilisitic
approaches like similarity propegation, or probabilistic models including
generative frameworks, or simply a hybrid approach.&lt;/p&gt;

&lt;p&gt;Generative probabilistic approaches are based on directed models, where
dependecies match decisions in a generative manner. There are a variety of
approaches, notably based on LDA and Bayesian Networks. Undirected
probabilistic approaches use semantics based on Markov Networks, to the
advantage that this allows a declarative syntax based on first-order logic
to create constraints. Several suggested approaches include Conditional
Random Fields, Markov Logic Networks (MLNs), and Probabilistic Soft Logic.&lt;/p&gt;

&lt;p&gt;Probabilistic soft logic introduces &lt;em&gt;reverse predicate equivalence&lt;/em&gt; meaning
that the same relation with the same entity gives evidence of two entities
being the same. This is not true logically, but can allow us to predict
the probability of a match with truth values in [0,1]. The declarative
language is used to define a constrained continuous Markov random field
in first order logic, and we can use relaxed logical operators. This has
significant implication for scaling improvements.&lt;/p&gt;

&lt;h2 id=&quot;scaling-to-big-data&quot;&gt;Scaling to Big Data&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Blocking/Canopy Generation&lt;/strong&gt;: The first approach to scaling to big data
is to use a blocking approach. Consider a Naïve pariwise comparsion
with 1,000 business mentions from 1,000 citties. This is &lt;em&gt;1 trillion&lt;/em&gt;
comparisons, which is 11.6 days for a microsecond comparison! However, we
know that business mentions are probably city-specific, therefore only
comparing mentions within cities reduces the comparison space to the more
managable &lt;em&gt;1 billion&lt;/em&gt; comparisions, which takes only 16 minutes at the same
rate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/blocking.png&quot; title=&quot;Blocking Heuristic&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/blocking.png&quot; alt=&quot;Blocking Heuristic&quot; title=&quot;Blocking Heuristic&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Although some matches can be missed using blocking techniques, the key is
to select a blocking algorithm that minimizes the subtraction of the set
of matching pair records and those satisfying the blocking criterion as in
the Venn diagram above, and maximizes the intersection. Common blocking
algorithms include hash based blocking, similarity or neighborhood based
blocking, or creating complex blocking predicates by combining simple
blocking predicates. Another powerful technique is minHash or locality
sensitive hashing, which uses distance measures and performs very well, and
is recommended as the state of the art for blocking.&lt;/p&gt;

&lt;p&gt;The final approach to blocking is more inline with the clustering methods
of earlier approaches. In &lt;em&gt;Canopy Clustering&lt;/em&gt; a distance metric is selected
with two thresholds. By picking a random mention, we create a canopy using
the first threshold and we remove all mentions where the distance from the
centroid is less than the second threshold. We continue this process so
long as the set M is not empty. This approach is interesting because
mentions can be included in more than one canopy, and therefore reduces the
chance that the blocking method excludes an actual match.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/images/canopy.png&quot; title=&quot;Canopy Clustering&quot;&gt;&lt;img src=&quot;http://www.cs.umd.edu/~bengfort/images/canopy.png&quot; alt=&quot;Canopy Clustering&quot; title=&quot;Canopy Clustering&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributed ER&lt;/strong&gt;: The MapReduce framework is very popular for large,
parallel tasks and there are several open source frameworks related to
Hadoop to apply to your application. MapReduce can be used with disjoint
blocking- e.g. in the Map Phase (per record computation) you compute the
blocks and associate with various reducers, and in the reduce phase (global
computation) you can perform the pairwise matching. Several other challenges
relating to implementations with MapReduce are discussed in the text and
can be reviewed during implementations of ER in MapReduce.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Entity resolution is becoming an increasingly important task as linked data
grows, and the requirement for graph based reasoning extends beyond
theoritical applications. With the advent of big data computations, this
need has become even more prevalent. Much research has been done in the
area in seperately named fields, and the tutorial by Dr. Getoor and Dr.
Machanavajjhala succinctly highlight the current state of the art and gives
a general sense of future work.&lt;/p&gt;

&lt;p&gt;Specifically, there needs to be a unified approach to the theory which can
give relational learning bounds. Since Entity Resolution is often part of
bigger inference applications, there needs to be a joint approach to
information extraction, and a characterization of how the success (or
errors) affects larger reasoning quality. Similarly, there is a need for
large, real-world datasets with ground truth to establish benchmarks for
performance.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;This has been a summary of the work done by Dr. Lise Getoor and Dr. Ashwin
Machanavajjhala for their Tutorial entitled &lt;em&gt;Entity Resolution for Big Data&lt;/em&gt;,
accepted at KDD 2013 in Chicago, Il.&lt;/p&gt;

&lt;p&gt;You can find the complete tutorial slides at:
&lt;a href=&quot;http://www.cs.umd.edu/~getoor/Tutorials/ER_KDD2013.pdf&quot; title=&quot;Tutorial Slides&quot;&gt;http://www.cs.umd.edu/~getoor/Tutorials/ER_KDD2013.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Special thanks to Lise and Ashwin for hosting Cobrain during their
run-through of the presentation.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/tutorials/entity-resolution-for-big-data/&quot;&gt;Entity Resolution for Big Data&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on March 10, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Goal Driven Mixed-Initiative Systems with Collaborative Filtering]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering/" />
  <id>http://www.cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering</id>
  <updated>2014-02-27 15:32:10 UTCT00:00:00-00:00</updated>
  <published>2014-02-27T00:00:00-05:00</published>
  
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Mixed-initiative systems combine the efforts of human actors with artificial agents to improve the performance of both participants in some mutual functionality. For example, an intelligent search application can be viewed as a mixed-initiative system where a human uses token strings or a special query language to enact searches that are carried out by an ensemble of statistical models which deliver intelligent results. In return, they benefit from the positive feedback of the user, e.g. which links are clicked, or how many queries it takes to find a result. &lt;/p&gt;

&lt;p&gt;A mixed-initiative system is said to be goal-driven when predefined goals are used to affect the behavior of both the human user and the underlying agent. In our search example, we could talk about an “academic goal” where the user is specifically looking for research materials. In this case, the types of queries made by the user will be influenced by this goal, and the results or domain of the agent should also take this goal into account. Additionally, the agent should seek to recover from failures or anomalies by generating its own goals. By using goals, we can effectively communicate bidirectional intent as well as explain failure in mixed-initiative systems. &lt;/p&gt;

&lt;p&gt;A third, interesting dimension comes into play when there are many human users participating in a mixed-initiative system. Namely, user behavior coupled with their goals creates a latent semantic network which can be accessed to improve mixed-initiative interactions through recommendation algorithms. In the next sections, I will describe the novel application of collaborative filtering algorithms improved with goal-dimensionality to mixed-initiative systems to both allow agents to provide other human help to a human user (goal combination) as well to elicit or change the goals of a human user in real time to improve their efficacy (goal generation). &lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering-for-goal-combination&quot;&gt;Collaborative Filtering for Goal Combination&lt;/h2&gt;

&lt;p&gt;Consider an opening scene from the 1970s TV version of Mission Impossible: Peter Graves pours over the dossiers of IMF agents as the camera hones in on their skills: weapons-expert, master of disguise, a navy diver. He carefully selects the perfect team for his mission as the burnt remnants of the mission tape smolder on the table. He knows that he will only succeed in his mission if he combines the expertise of his agent in a specific mission-oriented configuration.&lt;/p&gt;

&lt;p&gt;So too can our goal driven, mixed-initiative systems assist human users on their efforts, particularly in multi-domain systems. Each human user (or even possibly agent or model) has an expertise that is expressed through the goals of each user. A financial analyst will have goals related to their particular expertise, just like a language expert might also have goals related to a particular region. However, because a human user has many, changing goals- we can build a goal-to-goal similarity matrix using the goal-vector of the user; this matrix is the basis of item-centric collaborative filtering algorithms. &lt;/p&gt;

&lt;p&gt;The goal similarity matrix will produce clusters of related goals. The system can then use the goal cluster to discover other human analysts to recommend to the original user either in a passive context (provide hints based on the behavior of the other similar users) or in an active context- directly connecting the recommended user to the original user. Team selection can also be made on this basis- selection of members from related clusters to improve the overall effectiveness of the human team. Because recommended users will have different goals than the original user, this activity can be seen as goal combination. Either way, the human user will experience highly personalized guidance on part of the system through these recommendations.&lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering-for-goal-generation&quot;&gt;Collaborative Filtering for Goal Generation&lt;/h2&gt;

&lt;p&gt;As mentioned in the previous section, a human analyst (or agent’s) goals change over time- either as (1) the result of a new mission, (2) a change in world knowledge or the environment, or (3) in order to explain or recover from failure. It would be useful for a mixed-initiative system to anticipate these changes in goals particularly in order to adapt to (2) or (3). This will lead to a much smoother transition between goals, as well as improve the efficacy of a system that has to respond to a dynamic environment.&lt;/p&gt;

&lt;p&gt;Changing the goals of the analyst in real-time is possible through the novel use of our goal-based collaborative filtering with an additional dimension- the changing state of the behavior of the user. Our similarity matrix is now not only goal-to-goal, but instead goal-to-goal-to-state. This similarity matrix can then be decomposed to a series of class based goal-to-state similarity matrices. As the state of the user changes (e.g. the nature or classification of the behavior in terms of nearest-similar goals) these more similar goals are presented to the user in order to influence the user’s behavior towards more fruitful behavior (or to allow the user to avoid potential pitfalls when they recognize an ineffective goal).&lt;/p&gt;

&lt;p&gt;In this manner, the system can affect the behavior of the user before the user is aware of the need for a change in goals. This results in a highly personalized system that is bidirectional. Not only can the system use the behavior state to recommend goals and influence human behavior, the system can also use the goals to modify the results based on positive feedback from other users with similar goals and similar behavior. For instance, the selection of a recommended goal is a positive feedback mechanism that can be used in active learning when computing goal-state similarity matrices. &lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/thoughts/goal-driven-mixed-initiative-systems-with-collaborative-filtering/&quot;&gt;Goal Driven Mixed-Initiative Systems with Collaborative Filtering&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on February 27, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Getting Moved In]]></title>
  <link rel="alternate" type="text/html" href="http://www.cs.umd.edu/~bengfort/news/moving-in/" />
  <id>http://www.cs.umd.edu/~bengfort/news/moving-in</id>
  <published>2014-01-16T19:59:24Z</published>
  <updated>2014-01-16T19:59:24Z</updated>
  <author>
    <name>Benjamin Bengfort</name>
    <uri>http://www.cs.umd.edu/~bengfort</uri>
    <email>bengfort@cs.umd.edu</email>
  </author>
  <content type="html">&lt;p&gt;Hi, I’m Ben. Today, for the fourth time, I’m starting over on my PhD. I’ve
moved into my office in A.V. Williams, and I’m feeling like a veteran
entering an extended tour. But I’m here, I’m ready to work, so let’s get
researching!&lt;/p&gt;

&lt;p&gt;Also, I do Python.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#=&amp;gt; prints &amp;#39;[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]&amp;#39; to STDOUT.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I build this site with &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;, which builds the static site from
Markdown in a repository.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.cs.umd.edu/~bengfort/news/moving-in/&quot;&gt;Getting Moved In&lt;/a&gt; was originally published by Benjamin Bengfort at &lt;a href=&quot;http://www.cs.umd.edu/~bengfort&quot;&gt;Benjamin Bengfort&lt;/a&gt; on January 16, 2014.&lt;/p&gt;</content>
</entry>

</feed>
