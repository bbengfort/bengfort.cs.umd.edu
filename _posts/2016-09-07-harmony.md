---
layout: post
title:  "Reading &ldquo;Harmony: Towards Automated Self-Adaptive Consistency in Cloud Storage&rdquo;"
#modified: 2016-08-30 10:41:08 -0400
tags: [harmony, distributed systems, papers, cloud, cassandra]
category: scientific-reading
comments:
share:
---

> <small>H.-E. Chihoub, S. Ibrahim, G. Antoniu, and M. S. Perez, “Harmony: Towards automated self-adaptive consistency in cloud storage,” in 2012 IEEE International Conference on Cluster Computing, 2012, pp. 293–301.</small>

## Abstract ##

<small>In just a few years cloud computing has become a very popular paradigm and a business success story, with storage being one of the key features. To achieve high data availability, cloud storage services rely on replication. In this context, one major challenge is data consistency. In contrast to traditional approaches that are mostly based on strong consistency, many cloud storage services opt for weaker consistency models in order to achieve better availability and performance. This comes at the cost of a high probability of stale data being read, as the replicas involved in the reads may not always have the most recent write. In this paper, we propose a novel approach, named Harmony, which adaptively tunes the consistency level at run-time according to the application requirements. The key idea behind Harmony is an intelligent estimation model of stale reads, allowing to elastically scale up or down the number of replicas involved in read operations to maintain a low (possibly zero) tolerable fraction of stale reads. As a result, Harmony can meet the desired consistency of the applications while achieving good performance. We have implemented Harmony and performed extensive evaluations with the Cassandra cloud storage on Grid’5000 testbed and on Amazon EC2. The results show that Harmony can achieve good performance without exceeding the tolerated number of stale reads. For instance, in contrast to the static eventual consistency used in Cassandra, Harmony reduces the stale data being read by almost 80% while adding only minimal latency. Meanwhile, it improves the throughput of the system by 45% while maintaining the desired consistency requirements of the applications when compared to the strong consistency model in Cassandra.</small>

## Critical Reading ##

The Harmony paper is the second in the series of posts I'm writing that consider adaptive, hybrid, or continuous consistency models particularly with respect to the research I'm currently pursuing on Federated consistency.

Chihoub et al. start with the novel premise that while replication and eventual consistency provide high availability, the performance of such systems (Dynamo, Cassandra, BigTable, PNUTS, and HBase) is too costly in terms of inconsistency. They first cite a statistic that a single hour of down time for a credit card authorization system could cost between $2.2M-3.1M. However, under heavy reads and writes some systems may return up to 66.61% of stale reads. This is alarming because the likelihood is that two out of three reads are useless. They present therefore a consistency model that focuses on stale reads as the primary inconsistency metric and propose an adaptive mechanism called Harmony to address stale reads.

Their solution is to create an adaptive framework that sits on top of Cassandra. Applications can specify consistency in terms of the % of stale reads they are willing to tolerate: 0% stale reads means sequential consistency and 100% stale reads means letting the reigns go on eventual consistency. Most applications will choose some percent in the middle, however defining consistency this way allows applications to specify consistency along a continuum. Once specified, Harmony takes advantage of a Cassandra-specific feature that allows each access to specify the level of consistency. Normal Cassandra consistency settings are ONE or ALL, but you can also specify the number of nodes in the quorum required for an access.  Through real time monitoring of average read and write latencies, the system comes up with a likelihood of a stale read via a Poisson process model and uses that likelihood to compute the number of replicas required for the access such that the number of stale reads given the current likelihood is less than or equal to the application setting.

In evaluating their framework, Chihoub et al. showed that they could split the middle ground between "eventual" (Cassandra ONE consistency) and "strong" (Cassandra ALL consistency) by using Harmony-80% and Harmon-40% settings, thereby allowing consistency tuning. However, they also showed that as the number of threads increases, throughput decreases so that the eventual systems become as bad as the strong systems. Potentially this has to do with the implementation (Python 2.7) of adaptive consistency not being able to keep up, or perhaps it indicates a systematic problem in larger distributed systems. Overall, stale reads is an interesting approach to minimizing inconsistencies, and I think this paper did well to show adaptivity in real time. However, this model only works in Cassandra and Dynamo, and no consistency checking in terms of ordering was done.
